{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to TorchVision\n",
    "\n",
    "TorchVision is a package which consists of popular datasets, models and computer vision utilities such as transforms, display and writing videos/images, etc.\n",
    "\n",
    "We have already used some of the torchvision functionality in previous sections. In this section we will discuss them in more detail so that you are better equipped to use it in your work.\n",
    "\n",
    "Torchvision consists of the following classes:\n",
    "1. Datasets\n",
    "2. Transforms\n",
    "3. Models\n",
    "4. Utils\n",
    "5. IO\n",
    "6. Ops\n",
    "\n",
    "The most used of the above are `Datasets`, `Transforms` and `Models`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, here's the full markdown code with added necessary colors for the attached image. Please note that the colors will only be visible if your Markdown viewer supports inline HTML:\n",
    "\n",
    "\n",
    "# Datasets\n",
    "\n",
    "While loading datasets like MNIST, Fashion-MNIST etc. in previous sessions, we saw how useful they are. Many more datasets come packaged with TorchVision and are very popular. The `datasets` are mostly used with `dataloaders` available in PyTorch.\n",
    "\n",
    "## Function Syntax\n",
    "\n",
    "```python\n",
    "torchvision.datasets.DATASET(root, train=True, transform=None, target_transform=None, download=False)\n",
    "```\n",
    "\n",
    "Where,\n",
    "\n",
    "- `DATASET` is the name of the dataset, which can be MNIST, FashionMNIST, COCO etc. Get the full list here\n",
    "- `root` is the folder that stores the dataset. Use this if you opt to download.\n",
    "- `train` is a flag that specifies whether you should use the train data or test data.\n",
    "- `download` is a flag which is turned on when you want to download the data. Note that the data is not downloaded if it is already present in the root folder mentioned above.\n",
    "- `transform` applies a series of image transforms on the input images. For example, cropping, resizing, etc.\n",
    "- `target_transform` takes the target or labels and transforms it as required.\n",
    "\n",
    "<span style=\"color:blue\">## Why Is It Useful?</span>\n",
    "\n",
    "Suppose you are working on a problem and achieve a decent accuracy. Now, you want to test your model on different/harder data. So, you will have to search for a dataset, go through it then see how it is organized. Next, download it on your system and prepare it to fit your training pipeline. Only then will you be ready to use the new dataset.\n",
    "\n",
    "But when you use TorchVision datasets, <span style=\"color:green\">you can skip all these steps</span>, and treat new dataset as drop-in replacement for old one. <span style=\"color:red\">That's because almost all datasets available on torchvision have similar API.</span>\n",
    "\n",
    "They have common arguments like <span style=\"color:purple\">transform</span> and <span style=\"color:purple\">target_transform</span>, which transform input as well as labels/targets.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms\n",
    "\n",
    "These are image transforms applied while training a network. Simple operations like cropping, resizing and normalizing are all examples of a transform. Apply multiple transforms to an image, by chaining the transforms using the Compose Class.\n",
    "\n",
    "To see the different transforms available in TorchVision, click here.\n",
    "\n",
    "Some frequently used transforms:\n",
    "\n",
    "- <span style=\"color:blue\">`torchvision.transforms.ToTensor`</span> - It takes in a PIL image of dimension [H x W x C] in the range [0,255] and converts it to a float Tensor of dimension [C x H x W] in the range [0,1].\n",
    "- <span style=\"color:red\">`torchvision.transforms.Compose`</span> - It chains many transformers together so that you can apply then all in one go.\n",
    "\n",
    "Apart from these readymade transforms, there are <span style=\"color:green\">functional transforms</span>, which give you more control over the transformations. Read more\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "Just like `torchvision.datasets` consists of popular datasets used for experimentation, `torchvision.models` has many well-known models for computer vision tasks. For example:\n",
    "\n",
    "- Classification\n",
    "- Detection\n",
    "- Segmentation\n",
    "- Video Classification\n",
    "\n",
    "The list keeps growing with time.\n",
    "\n",
    "## Function Syntax\n",
    "\n",
    "```python\n",
    "model = torchvision.models.MODEL(pretrained=True)\n",
    "```\n",
    "\n",
    "Where,\n",
    "\n",
    "- `MODEL` is the name of the model such as AlexNet, ResNet etc. Check the full list of available models [here](#).\n",
    "- `pretrained` is the flag which specifies whether you want the model to be initialized with the pretrained weights of the model or not. If set to True, it will also download the weights file, when absent.\n",
    "\n",
    "You can use these models for similar problems. Or a subclass of the problem for which the pre-trained model was trained. You can even treat these models as starting point for fine-tuning your model to perform a new task. More on fine-tuning later.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "\n",
    "It has 2 nice functions which come in handy while dealing with images and publishing findings of your work.\n",
    "\n",
    "## Make grid of images for display\n",
    "\n",
    "```python\n",
    "torchvision.utils.make_grid(tensor, nrow=8, padding=2, normalize=False, range=None, scale_each=False, pad_value=0)\n",
    "```\n",
    "\n",
    "Where,\n",
    "\n",
    "- `tensor` – 4D mini-batch Tensor of shape (B x C x H x W) or a list of images all of the same size.\n",
    "- `nrow` – Number of images displayed in each row of the grid. The final grid size is (B / nrow, nrow). Default: 8.\n",
    "- `padding` – amount of padding. Default: 2.\n",
    "- `normalize` – If True scales the image to the range (0, 1), by the min and max values specified by range. Default: False.\n",
    "- `range` – tuple (min, max) where min and max are numbers, then these numbers are used to normalize the image. Default: None.\n",
    "- `scale_each` – If True, scale each image in the batch of images separately rather than (the min, max) over all images. Default: False.\n",
    "- `pad_value` – Value for the padded pixels. Default: 0.\n",
    "\n",
    "## Save Image\n",
    "\n",
    "```python\n",
    "torchvision.utils.save_image(tensor, fp, nrow=8, padding=2, normalize=False, range=None, scale_each=False, pad_value=0, format=None)\n",
    "```\n",
    "\n",
    "If you provide a mini-batch to the above function, it saves them as a grid of images. The other arguments are similar to make_grid.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IO\n",
    "\n",
    "As the name suggests, it is designed to perform IO operations such as reading/writing media files. Currently it only supports video reading and writing.\n",
    "\n",
    "## Read Video\n",
    "\n",
    "```python\n",
    "torchvision.io.read_video(filename, start_pts=0, end_pts=None, pts_unit='pts')\n",
    "```\n",
    "\n",
    "Where,\n",
    "\n",
    "- `filename` – The name of the video file to read.\n",
    "- `start_pts` – The start presentation timestamp of the video to read. Default: 0.\n",
    "- `end_pts` – The end presentation timestamp of the video to read. Default: None.\n",
    "- `pts_unit` – The unit of the presentation timestamps (pts) in `start_pts` and `end_pts`. Default: 'pts'.\n",
    "\n",
    "It reads a video from filename and returns the video as well as audio frames. You can also specify the time stamp from/to where you want to read the video.\n",
    "\n",
    "## Write Video\n",
    "\n",
    "```python\n",
    "torchvision.io.write_video(filename, video_array, fps, video_codec='libx264', options=None)\n",
    "```\n",
    "\n",
    "Where,\n",
    "\n",
    "- `filename` – The name of the video file to write.\n",
    "- `video_array` – The array of video frames to write.\n",
    "- `fps` – The frames per second (fps) rate for the written video.\n",
    "- `video_codec` – The codec to use for the written video. Default: 'libx264'.\n",
    "- `options` – Additional options to pass to the video writer. Default: None.\n",
    "\n",
    "If you provide a mini-batch to the above function, it saves them as a grid of images. The other arguments are similar to make_grid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ops\n",
    "\n",
    "The ops module implements some functions used for specific computer vision tasks. Some of them are:\n",
    "\n",
    "- <span style=\"color:lightblue\">Non Maximum suppression</span> - Used in Object detection pipelines\n",
    "- <span style=\"color:red\">Region of Interest Pooling</span> - Used in Fast RCNN paper\n",
    "- <span style=\"color:green\">Region of Interest Alignment</span> - Used in Mask RCNN paper\n",
    "\n",
    "These are just mentioned for the sake of completeness and are rarely used.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
