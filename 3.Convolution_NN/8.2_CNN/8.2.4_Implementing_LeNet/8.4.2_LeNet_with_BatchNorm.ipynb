{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='pink'>Table of contents</font>\n",
    "\n",
    "- [LeNet5 Architecture](#lenet)\n",
    "- [Display the Network](#display)\n",
    "- [Get the Fashion-MNIST Data](#get-data)\n",
    "- [System Configuration](#sys-config)\n",
    "- [Training Configuration](#train-config)\n",
    "- [System Setup](#sys-setup)\n",
    "- [Training](#training)\n",
    "- [Validation](#validation)\n",
    "- [Main function](#main)\n",
    "- [Plot Loss](#plot-loss)\n",
    "- [Miscellaneous](#misc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font style=\"color:pink\">Convolutional Neural Network Using Batch Normalization</font>\n",
    "\n",
    "In this notebook, we  add batch norm layers to the LeNet network, and see how it affects network training and convergence.\n",
    "\n",
    "Instead of the MNIST dataset, which overfits easily, we will use the Fashion MNIST dataset.\n",
    "\n",
    "The figure below shows some samples from the Fashion MNIST dataset.\n",
    "\n",
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2021/01/c3-w3-fashion-mnist-sprite.jpg\" width=\"600\">\n",
    "\n",
    "There are 10 classes. Each training and testing example is assigned to one of the following labels:\n",
    "\n",
    "| Label | Description |\n",
    "| --- | --- |\n",
    "| 0 | T-shirt/top |\n",
    "| 1 | Trouser |\n",
    "| 2 | Pullover |\n",
    "| 3 | Dress |\n",
    "| 4 | Coat |\n",
    "| 5 | Sandal |\n",
    "| 6 | Shirt |\n",
    "| 7 | Sneaker |\n",
    "| 8 | Bag |\n",
    "| 9 | Ankle boot |\n",
    "\n",
    "\n",
    "\n",
    "We want to classify images in this dataset, using the LeNet network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # one of the best graphics library for python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "from typing import Iterable\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">1. LeNet Architecture with BatchNorm</font><a name=\"lenet\"></a>\n",
    "\n",
    "We have already explained the architecture for LeNet in the previous notebook.\n",
    "\n",
    "Here, we create another model called LeNetBN, adding Batch Normalization layers to the 2 convolution blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolution layers\n",
    "        self._body = nn.Sequential(\n",
    "            # First convolution Layer\n",
    "            # input size = (32, 32), output size = (28, 28)\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Max pool 2-d\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            # Second convolution layer\n",
    "            # input size = (14, 14), output size = (10, 10)\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # output size = (5, 5)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self._head = nn.Sequential(\n",
    "            # First fully connected layer\n",
    "            # in_features = total number of weights in last conv layer = 16 * 5 * 5\n",
    "            nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # second fully connected layer\n",
    "            # in_features = output of last linear layer = 120 \n",
    "            nn.Linear(in_features=120, out_features=84), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Third fully connected layer which is also output layer\n",
    "            # in_features = output of last linear layer = 84\n",
    "            # and out_features = number of classes = 10 (MNIST data 0-9)\n",
    "            nn.Linear(in_features=84, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply feature extractor\n",
    "        x = self._body(x)\n",
    "        # flatten the output of conv layers\n",
    "        # dimension should be batch_size * number_of weight_in_last conv_layer\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # apply classification head\n",
    "        x = self._head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNetBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # convolution layers\n",
    "        self._body = nn.Sequential(\n",
    "            # First convolution Layer\n",
    "            # input size = (32, 32), output size = (28, 28)\n",
    "            nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5),\n",
    "            nn.BatchNorm2d(6),\n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Max pool 2-d\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            # Second convolution layer\n",
    "            # input size = (14, 14), output size = (10, 10)\n",
    "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            # output size = (5, 5)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self._head = nn.Sequential(\n",
    "            # First fully connected layer\n",
    "            # in_features = total number of weight in last conv layer = 16 * 5 * 5\n",
    "            nn.Linear(in_features=16 * 5 * 5, out_features=120), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # second fully connected layer\n",
    "            # in_features = output of last linear layer = 120 \n",
    "            nn.Linear(in_features=120, out_features=84), \n",
    "            \n",
    "            # ReLU activation\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Third fully connected layer. It is also output layer\n",
    "            # in_features = output of last linear layer = 84\n",
    "            # and out_features = number of classes = 10 (MNIST data 0-9)\n",
    "            nn.Linear(in_features=84, out_features=10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # apply feature extractor\n",
    "        x = self._body(x)\n",
    "        # flatten the output of conv layers\n",
    "        # dimension should be batch_size * number_of weights_in_last conv_layer\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # apply classification head\n",
    "        x = self._head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">2. Display the Network</font><a name=\"display\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (_body): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (_head): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "LeNetBN(\n",
      "  (_body): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (_head): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "lenet_model = LeNet()\n",
    "print(lenet_model)\n",
    "lenetBN_model = LeNetBN()\n",
    "print(lenetBN_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## <font style=\"color:green\">3. Get Fashion-MNIST Data</font><a name=\"get-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(batch_size, data_root='data', num_workers=1):\n",
    "    \n",
    "    train_test_transforms = transforms.Compose([\n",
    "        # Resize to 32X32\n",
    "        transforms.Resize((32, 32)),\n",
    "        # this re-scales image tensor values between 0-1. image_tensor /= 255\n",
    "        transforms.ToTensor(),\n",
    "        # subtract mean (0.2860) and divide by variance (0.3530).\n",
    "        # This mean and variance is calculated on training data (verify for yourself)\n",
    "        transforms.Normalize((0.2860, ), (0.3530, ))\n",
    "    ])\n",
    "    \n",
    "    # train dataloader\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST(root=data_root, train=True, download=True, transform=train_test_transforms),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    # test dataloader\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST(root=data_root, train=False, download=True, transform=train_test_transforms),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">4. System Configuration</font><a name=\"sys-config\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SystemConfiguration:\n",
    "    '''\n",
    "    Describes the common system setting needed for reproducible training\n",
    "    '''\n",
    "    seed: int = 42  # seed number to set the state of all random number generators\n",
    "    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n",
    "    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">5. Training Configuration</font><a name=\"train-config\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TrainingConfiguration:\n",
    "    '''\n",
    "    Describes configuration of the training process\n",
    "    '''\n",
    "    batch_size: int = 32  # amount of data to pass through the network at each forward-backward iteration\n",
    "    epochs_count: int = 20  # number of times the whole dataset will be passed through the network\n",
    "    learning_rate: float = 0.01  # determines the speed of network's weights update\n",
    "    log_interval: int = 100  # how many batches to wait between logging training status\n",
    "    test_interval: int = 1  # how many epochs to wait before another test. Set to 1 to get val loss at each epoch\n",
    "    data_root: str = \"data\"  # folder to save MNIST data (default: data)\n",
    "    num_workers: int = 10  # number of concurrent processes used to prepare data\n",
    "    device: str = 'cuda'  # device to use for training.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">6. System Setup</font><a name=\"sys-setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_system(system_config: SystemConfiguration) -> None:\n",
    "    torch.manual_seed(system_config.seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n",
    "        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">7. Training</font><a name=\"training\"></a>\n",
    "We are familiar with the training pipeline used in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n",
    "    train_loader: torch.utils.data.DataLoader, epoch_idx: int\n",
    ") -> None:\n",
    "    \n",
    "    # change model in training mode\n",
    "    model.train()\n",
    "    \n",
    "    # to get batch loss\n",
    "    batch_loss = np.array([])\n",
    "    \n",
    "    # to get batch accuracy\n",
    "    batch_acc = np.array([])\n",
    "        \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        # clone target\n",
    "        indx_target = target.clone()\n",
    "        # send data to device (its is medatory if GPU has to be used)\n",
    "        data = data.to(train_config.device)\n",
    "        # send target to device\n",
    "        target = target.to(train_config.device)\n",
    "\n",
    "        # reset parameters gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass to the model\n",
    "        output = model(data)\n",
    "        \n",
    "        # cross entropy loss\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        \n",
    "        # find gradients w.r.t training parameters\n",
    "        loss.backward()\n",
    "        # Update parameters using gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = np.append(batch_loss, [loss.item()])\n",
    "        \n",
    "        # get probability score using softmax\n",
    "        prob = F.softmax(output, dim=1)\n",
    "            \n",
    "        # get the index of the max probability\n",
    "        pred = prob.data.max(dim=1)[1]  \n",
    "                        \n",
    "        # correct prediction\n",
    "        correct = pred.cpu().eq(indx_target).sum()\n",
    "            \n",
    "        # accuracy\n",
    "        acc = float(correct) / float(len(data))\n",
    "        \n",
    "        batch_acc = np.append(batch_acc, [acc])\n",
    "\n",
    "        if batch_idx % train_config.log_interval == 0 and batch_idx > 0:              \n",
    "            print(\n",
    "                'Train Epoch: {} [{}/{}] Loss: {:.6f} Acc: {:.4f}'.format(\n",
    "                    epoch_idx, batch_idx * len(data), len(train_loader.dataset), loss.item(), acc\n",
    "                )\n",
    "            )\n",
    "            \n",
    "    epoch_loss = batch_loss.mean()\n",
    "    epoch_acc = batch_acc.mean()\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">8. Validation</font><a name=\"validation\"></a>\n",
    "\n",
    "After every few epochs **`validation`** is called, with the `trained model` and `test_loader` to get validation loss and accuracy.\n",
    "\n",
    "**Note:** We use `model.eval()` to enable evaluation mode of the model. This will stop calculating the running estimate of mean and variance of data. Using instead just the mean and variance computed while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(\n",
    "    train_config: TrainingConfiguration,\n",
    "    model: nn.Module,\n",
    "    test_loader: torch.utils.data.DataLoader,\n",
    ") -> float:\n",
    "    # \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    count_corect_predictions = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            indx_target = target.clone()\n",
    "            data = data.to(train_config.device)\n",
    "\n",
    "            target = target.to(train_config.device)\n",
    "\n",
    "            output = model(data)\n",
    "            # add loss for each mini batch\n",
    "            test_loss += F.cross_entropy(output, target).item()\n",
    "\n",
    "            # get probability score using softmax\n",
    "            prob = F.softmax(output, dim=1)\n",
    "\n",
    "            # get the index of the max probability\n",
    "            pred = prob.data.max(dim=1)[1] \n",
    "\n",
    "            # add correct prediction count\n",
    "            count_corect_predictions += pred.cpu().eq(indx_target).sum()\n",
    "\n",
    "        # average over number of mini-batches\n",
    "        test_loss = test_loss / len(test_loader)  \n",
    "\n",
    "        # average over number of dataset\n",
    "        accuracy = 100. * count_corect_predictions / len(test_loader.dataset)\n",
    "\n",
    "        print(\n",
    "            '\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "                test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n",
    "            )\n",
    "        )\n",
    "    return test_loss, accuracy/100.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">9. Main</font><a name=\"main\"></a>\n",
    "\n",
    "\n",
    "Here, we use the configuration parameters defined above and start  training. \n",
    "\n",
    "1. Set up system parameters like CPU/GPU, number of threads etc.\n",
    "1. Load the data using dataloaders.\n",
    "1. Create an instance of the LeNet model.\n",
    "1. Specify optimizer to use.\n",
    "1. Set up variables to track loss and accuracy and start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, system_configuration=SystemConfiguration(), training_configuration=TrainingConfiguration()):\n",
    "    \n",
    "    # system configuration\n",
    "    setup_system(system_configuration)\n",
    "\n",
    "    # batch size\n",
    "    batch_size_to_set = training_configuration.batch_size\n",
    "    # num_workers\n",
    "    num_workers_to_set = training_configuration.num_workers\n",
    "    # epochs\n",
    "    epoch_num_to_set = training_configuration.epochs_count\n",
    "\n",
    "    # if GPU is available use training config, \n",
    "    # else lower batch_size, num_workers and epochs count\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        batch_size_to_set = 16\n",
    "        num_workers_to_set = 2\n",
    "        epoch_num_to_set = 10\n",
    "\n",
    "    # data loader\n",
    "    train_loader, test_loader = get_data(\n",
    "        batch_size=batch_size_to_set,\n",
    "        data_root=training_configuration.data_root,\n",
    "        num_workers=num_workers_to_set\n",
    "    )\n",
    "    \n",
    "    # Update training configuration\n",
    "    training_configuration = TrainingConfiguration(\n",
    "        device=device,\n",
    "        epochs_count=epoch_num_to_set,\n",
    "        batch_size=batch_size_to_set,\n",
    "        num_workers=num_workers_to_set\n",
    "    )\n",
    "        \n",
    "    # send model to device (GPU/CPU)\n",
    "    model.to(training_configuration.device)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=training_configuration.learning_rate\n",
    "    )\n",
    "\n",
    "    best_loss = torch.tensor(np.inf)\n",
    "    \n",
    "    # epoch train/test loss\n",
    "    epoch_train_loss = np.array([])\n",
    "    epoch_test_loss = np.array([])\n",
    "    \n",
    "    # epch train/test accuracy\n",
    "    epoch_train_acc = np.array([])\n",
    "    epoch_test_acc = np.array([])\n",
    "    \n",
    "    # trainig time measurement\n",
    "    t_begin = time.time()\n",
    "    for epoch in range(training_configuration.epochs_count):\n",
    "        \n",
    "        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch)\n",
    "        \n",
    "        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n",
    "        \n",
    "        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n",
    "\n",
    "        elapsed_time = time.time() - t_begin\n",
    "        speed_epoch = elapsed_time / (epoch + 1)\n",
    "        speed_batch = speed_epoch / len(train_loader)\n",
    "        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n",
    "        \n",
    "        print(\n",
    "            \"Elapsed {:.2f}s, {:.2f} s/epoch, {:.2f} s/batch, ets {:.2f}s\".format(\n",
    "                elapsed_time, speed_epoch, speed_batch, eta\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if epoch % training_configuration.test_interval == 0:\n",
    "            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n",
    "            \n",
    "            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n",
    "        \n",
    "            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n",
    "            \n",
    "            if current_loss < best_loss:\n",
    "                best_loss = current_loss\n",
    "                \n",
    "    print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))\n",
    "    \n",
    "    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [05:52<00:00, 74888.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 85867.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:48<00:00, 91494.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 2957036.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/maneesh/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [3200/60000] Loss: 2.294390 Acc: 0.0625\n",
      "Train Epoch: 0 [6400/60000] Loss: 2.096482 Acc: 0.4688\n",
      "Train Epoch: 0 [9600/60000] Loss: 1.443131 Acc: 0.4375\n",
      "Train Epoch: 0 [12800/60000] Loss: 0.890218 Acc: 0.6250\n",
      "Train Epoch: 0 [16000/60000] Loss: 0.984824 Acc: 0.6875\n",
      "Train Epoch: 0 [19200/60000] Loss: 0.973815 Acc: 0.6562\n",
      "Train Epoch: 0 [22400/60000] Loss: 0.796188 Acc: 0.6875\n",
      "Train Epoch: 0 [25600/60000] Loss: 0.876855 Acc: 0.5938\n",
      "Train Epoch: 0 [28800/60000] Loss: 0.695319 Acc: 0.7188\n",
      "Train Epoch: 0 [32000/60000] Loss: 0.717891 Acc: 0.7812\n",
      "Train Epoch: 0 [35200/60000] Loss: 0.854460 Acc: 0.5938\n",
      "Train Epoch: 0 [38400/60000] Loss: 0.558328 Acc: 0.8438\n",
      "Train Epoch: 0 [41600/60000] Loss: 0.625811 Acc: 0.6875\n",
      "Train Epoch: 0 [44800/60000] Loss: 1.004174 Acc: 0.6250\n",
      "Train Epoch: 0 [48000/60000] Loss: 0.618101 Acc: 0.8125\n",
      "Train Epoch: 0 [51200/60000] Loss: 0.629146 Acc: 0.7500\n",
      "Train Epoch: 0 [54400/60000] Loss: 0.777719 Acc: 0.6562\n",
      "Train Epoch: 0 [57600/60000] Loss: 0.777114 Acc: 0.7500\n",
      "Elapsed 5.09s, 5.09 s/epoch, 0.00 s/batch, ets 96.72s\n",
      "\n",
      "Test set: Average loss: 0.6335, Accuracy: 7525/10000 (75%)\n",
      "\n",
      "Train Epoch: 1 [3200/60000] Loss: 0.756104 Acc: 0.6875\n",
      "Train Epoch: 1 [6400/60000] Loss: 0.532265 Acc: 0.7812\n",
      "Train Epoch: 1 [9600/60000] Loss: 0.565452 Acc: 0.6562\n",
      "Train Epoch: 1 [12800/60000] Loss: 0.727951 Acc: 0.7812\n",
      "Train Epoch: 1 [16000/60000] Loss: 0.509099 Acc: 0.8438\n",
      "Train Epoch: 1 [19200/60000] Loss: 0.797973 Acc: 0.6250\n",
      "Train Epoch: 1 [22400/60000] Loss: 0.411617 Acc: 0.8750\n",
      "Train Epoch: 1 [25600/60000] Loss: 0.468441 Acc: 0.8125\n",
      "Train Epoch: 1 [28800/60000] Loss: 0.434853 Acc: 0.8125\n",
      "Train Epoch: 1 [32000/60000] Loss: 0.380555 Acc: 0.8125\n",
      "Train Epoch: 1 [35200/60000] Loss: 0.295220 Acc: 0.9375\n",
      "Train Epoch: 1 [38400/60000] Loss: 0.565793 Acc: 0.7812\n",
      "Train Epoch: 1 [41600/60000] Loss: 0.498203 Acc: 0.7812\n",
      "Train Epoch: 1 [44800/60000] Loss: 0.602964 Acc: 0.6562\n",
      "Train Epoch: 1 [48000/60000] Loss: 0.627429 Acc: 0.7500\n",
      "Train Epoch: 1 [51200/60000] Loss: 0.387071 Acc: 0.8438\n",
      "Train Epoch: 1 [54400/60000] Loss: 0.743476 Acc: 0.7188\n",
      "Train Epoch: 1 [57600/60000] Loss: 0.592792 Acc: 0.7188\n",
      "Elapsed 10.03s, 5.01 s/epoch, 0.00 s/batch, ets 90.26s\n",
      "\n",
      "Test set: Average loss: 0.5189, Accuracy: 8057/10000 (81%)\n",
      "\n",
      "Train Epoch: 2 [3200/60000] Loss: 0.723252 Acc: 0.8750\n",
      "Train Epoch: 2 [6400/60000] Loss: 0.463854 Acc: 0.8438\n",
      "Train Epoch: 2 [9600/60000] Loss: 0.227499 Acc: 0.9375\n",
      "Train Epoch: 2 [12800/60000] Loss: 0.467658 Acc: 0.8750\n",
      "Train Epoch: 2 [16000/60000] Loss: 0.277860 Acc: 0.9375\n",
      "Train Epoch: 2 [19200/60000] Loss: 0.837468 Acc: 0.6875\n",
      "Train Epoch: 2 [22400/60000] Loss: 0.461072 Acc: 0.8750\n",
      "Train Epoch: 2 [25600/60000] Loss: 0.485455 Acc: 0.8438\n",
      "Train Epoch: 2 [28800/60000] Loss: 0.495470 Acc: 0.7500\n",
      "Train Epoch: 2 [32000/60000] Loss: 0.169891 Acc: 0.9375\n",
      "Train Epoch: 2 [35200/60000] Loss: 0.651106 Acc: 0.7188\n",
      "Train Epoch: 2 [38400/60000] Loss: 0.412415 Acc: 0.8438\n",
      "Train Epoch: 2 [41600/60000] Loss: 0.495883 Acc: 0.8438\n",
      "Train Epoch: 2 [44800/60000] Loss: 0.517807 Acc: 0.7812\n",
      "Train Epoch: 2 [48000/60000] Loss: 0.216547 Acc: 0.9688\n",
      "Train Epoch: 2 [51200/60000] Loss: 0.292046 Acc: 0.9062\n",
      "Train Epoch: 2 [54400/60000] Loss: 0.247903 Acc: 0.9375\n",
      "Train Epoch: 2 [57600/60000] Loss: 0.414633 Acc: 0.8125\n",
      "Elapsed 15.04s, 5.01 s/epoch, 0.00 s/batch, ets 85.24s\n",
      "\n",
      "Test set: Average loss: 0.4576, Accuracy: 8348/10000 (83%)\n",
      "\n",
      "Train Epoch: 3 [3200/60000] Loss: 0.417051 Acc: 0.8750\n",
      "Train Epoch: 3 [6400/60000] Loss: 0.290102 Acc: 0.9062\n",
      "Train Epoch: 3 [9600/60000] Loss: 0.476442 Acc: 0.9062\n",
      "Train Epoch: 3 [12800/60000] Loss: 0.704254 Acc: 0.8438\n",
      "Train Epoch: 3 [16000/60000] Loss: 0.328733 Acc: 0.8750\n",
      "Train Epoch: 3 [19200/60000] Loss: 0.425838 Acc: 0.8750\n",
      "Train Epoch: 3 [22400/60000] Loss: 0.728543 Acc: 0.7500\n",
      "Train Epoch: 3 [25600/60000] Loss: 0.317041 Acc: 0.9062\n",
      "Train Epoch: 3 [28800/60000] Loss: 0.442045 Acc: 0.8750\n",
      "Train Epoch: 3 [32000/60000] Loss: 0.243375 Acc: 0.9375\n",
      "Train Epoch: 3 [35200/60000] Loss: 0.344210 Acc: 0.9062\n",
      "Train Epoch: 3 [38400/60000] Loss: 0.225050 Acc: 0.9062\n",
      "Train Epoch: 3 [41600/60000] Loss: 0.414813 Acc: 0.8438\n",
      "Train Epoch: 3 [44800/60000] Loss: 0.231632 Acc: 0.9062\n",
      "Train Epoch: 3 [48000/60000] Loss: 0.735753 Acc: 0.6875\n",
      "Train Epoch: 3 [51200/60000] Loss: 0.993680 Acc: 0.7500\n",
      "Train Epoch: 3 [54400/60000] Loss: 0.628615 Acc: 0.6875\n",
      "Train Epoch: 3 [57600/60000] Loss: 0.378442 Acc: 0.9062\n",
      "Elapsed 20.04s, 5.01 s/epoch, 0.00 s/batch, ets 80.18s\n",
      "\n",
      "Test set: Average loss: 0.4180, Accuracy: 8473/10000 (85%)\n",
      "\n",
      "Train Epoch: 4 [3200/60000] Loss: 0.310867 Acc: 0.8750\n",
      "Train Epoch: 4 [6400/60000] Loss: 0.382371 Acc: 0.8750\n",
      "Train Epoch: 4 [9600/60000] Loss: 0.454300 Acc: 0.8438\n",
      "Train Epoch: 4 [12800/60000] Loss: 0.376933 Acc: 0.7812\n",
      "Train Epoch: 4 [16000/60000] Loss: 0.167565 Acc: 1.0000\n",
      "Train Epoch: 4 [19200/60000] Loss: 0.484336 Acc: 0.8438\n",
      "Train Epoch: 4 [22400/60000] Loss: 0.266267 Acc: 0.8750\n",
      "Train Epoch: 4 [25600/60000] Loss: 0.274625 Acc: 0.9062\n",
      "Train Epoch: 4 [28800/60000] Loss: 0.452678 Acc: 0.8438\n",
      "Train Epoch: 4 [32000/60000] Loss: 0.380880 Acc: 0.8125\n",
      "Train Epoch: 4 [35200/60000] Loss: 0.357347 Acc: 0.9062\n",
      "Train Epoch: 4 [38400/60000] Loss: 0.324877 Acc: 0.8438\n",
      "Train Epoch: 4 [41600/60000] Loss: 0.330172 Acc: 0.8750\n",
      "Train Epoch: 4 [44800/60000] Loss: 0.339705 Acc: 0.8125\n",
      "Train Epoch: 4 [48000/60000] Loss: 0.303611 Acc: 0.8750\n",
      "Train Epoch: 4 [51200/60000] Loss: 0.497572 Acc: 0.8750\n",
      "Train Epoch: 4 [54400/60000] Loss: 0.236896 Acc: 0.9375\n",
      "Train Epoch: 4 [57600/60000] Loss: 0.462956 Acc: 0.8125\n",
      "Elapsed 25.06s, 5.01 s/epoch, 0.00 s/batch, ets 75.17s\n",
      "\n",
      "Test set: Average loss: 0.4189, Accuracy: 8497/10000 (85%)\n",
      "\n",
      "Train Epoch: 5 [3200/60000] Loss: 0.459255 Acc: 0.9062\n",
      "Train Epoch: 5 [6400/60000] Loss: 0.360036 Acc: 0.8750\n",
      "Train Epoch: 5 [9600/60000] Loss: 0.236162 Acc: 0.9375\n",
      "Train Epoch: 5 [12800/60000] Loss: 0.414899 Acc: 0.8750\n",
      "Train Epoch: 5 [16000/60000] Loss: 0.408620 Acc: 0.8438\n",
      "Train Epoch: 5 [19200/60000] Loss: 0.389589 Acc: 0.8438\n",
      "Train Epoch: 5 [22400/60000] Loss: 0.383625 Acc: 0.8125\n",
      "Train Epoch: 5 [25600/60000] Loss: 0.192736 Acc: 0.9375\n",
      "Train Epoch: 5 [28800/60000] Loss: 0.547566 Acc: 0.7500\n",
      "Train Epoch: 5 [32000/60000] Loss: 0.290255 Acc: 0.9062\n",
      "Train Epoch: 5 [35200/60000] Loss: 0.257681 Acc: 0.8750\n",
      "Train Epoch: 5 [38400/60000] Loss: 0.386055 Acc: 0.8438\n",
      "Train Epoch: 5 [41600/60000] Loss: 0.217223 Acc: 0.9375\n",
      "Train Epoch: 5 [44800/60000] Loss: 0.242812 Acc: 0.8750\n",
      "Train Epoch: 5 [48000/60000] Loss: 0.666055 Acc: 0.6875\n",
      "Train Epoch: 5 [51200/60000] Loss: 0.231777 Acc: 0.9375\n",
      "Train Epoch: 5 [54400/60000] Loss: 0.314010 Acc: 0.9062\n",
      "Train Epoch: 5 [57600/60000] Loss: 0.286080 Acc: 0.9375\n",
      "Elapsed 30.05s, 5.01 s/epoch, 0.00 s/batch, ets 70.11s\n",
      "\n",
      "Test set: Average loss: 0.3952, Accuracy: 8583/10000 (86%)\n",
      "\n",
      "Train Epoch: 6 [3200/60000] Loss: 0.186307 Acc: 0.9375\n",
      "Train Epoch: 6 [6400/60000] Loss: 0.301688 Acc: 0.8438\n",
      "Train Epoch: 6 [9600/60000] Loss: 0.236536 Acc: 0.9375\n",
      "Train Epoch: 6 [12800/60000] Loss: 0.241633 Acc: 0.9062\n",
      "Train Epoch: 6 [16000/60000] Loss: 0.177215 Acc: 0.9688\n",
      "Train Epoch: 6 [19200/60000] Loss: 0.180413 Acc: 0.9688\n",
      "Train Epoch: 6 [22400/60000] Loss: 0.320284 Acc: 0.9062\n",
      "Train Epoch: 6 [25600/60000] Loss: 0.331646 Acc: 0.8438\n",
      "Train Epoch: 6 [28800/60000] Loss: 0.292314 Acc: 0.8125\n",
      "Train Epoch: 6 [32000/60000] Loss: 0.235318 Acc: 0.9062\n",
      "Train Epoch: 6 [35200/60000] Loss: 0.290330 Acc: 0.9062\n",
      "Train Epoch: 6 [38400/60000] Loss: 0.585048 Acc: 0.7812\n",
      "Train Epoch: 6 [41600/60000] Loss: 0.276581 Acc: 0.8438\n",
      "Train Epoch: 6 [44800/60000] Loss: 0.567839 Acc: 0.8438\n",
      "Train Epoch: 6 [48000/60000] Loss: 0.296563 Acc: 0.8438\n",
      "Train Epoch: 6 [51200/60000] Loss: 0.307307 Acc: 0.8438\n",
      "Train Epoch: 6 [54400/60000] Loss: 0.425274 Acc: 0.9062\n",
      "Train Epoch: 6 [57600/60000] Loss: 0.454532 Acc: 0.8125\n",
      "Elapsed 35.02s, 5.00 s/epoch, 0.00 s/batch, ets 65.03s\n",
      "\n",
      "Test set: Average loss: 0.3834, Accuracy: 8616/10000 (86%)\n",
      "\n",
      "Train Epoch: 7 [3200/60000] Loss: 0.289049 Acc: 0.8438\n",
      "Train Epoch: 7 [6400/60000] Loss: 0.689724 Acc: 0.7500\n",
      "Train Epoch: 7 [9600/60000] Loss: 0.388815 Acc: 0.8438\n",
      "Train Epoch: 7 [12800/60000] Loss: 0.221429 Acc: 0.9062\n",
      "Train Epoch: 7 [16000/60000] Loss: 0.314326 Acc: 0.8438\n",
      "Train Epoch: 7 [19200/60000] Loss: 0.614011 Acc: 0.8125\n",
      "Train Epoch: 7 [22400/60000] Loss: 0.312684 Acc: 0.8750\n",
      "Train Epoch: 7 [25600/60000] Loss: 0.442061 Acc: 0.8750\n",
      "Train Epoch: 7 [28800/60000] Loss: 0.353377 Acc: 0.8750\n",
      "Train Epoch: 7 [32000/60000] Loss: 0.452030 Acc: 0.8438\n",
      "Train Epoch: 7 [35200/60000] Loss: 0.415006 Acc: 0.8125\n",
      "Train Epoch: 7 [38400/60000] Loss: 0.371950 Acc: 0.8125\n",
      "Train Epoch: 7 [41600/60000] Loss: 0.310733 Acc: 0.8438\n",
      "Train Epoch: 7 [44800/60000] Loss: 0.249667 Acc: 0.8750\n",
      "Train Epoch: 7 [48000/60000] Loss: 0.375255 Acc: 0.9062\n",
      "Train Epoch: 7 [51200/60000] Loss: 0.233611 Acc: 0.9062\n",
      "Train Epoch: 7 [54400/60000] Loss: 0.256632 Acc: 0.9375\n",
      "Train Epoch: 7 [57600/60000] Loss: 0.209897 Acc: 0.8750\n",
      "Elapsed 39.87s, 4.98 s/epoch, 0.00 s/batch, ets 59.80s\n",
      "\n",
      "Test set: Average loss: 0.3509, Accuracy: 8700/10000 (87%)\n",
      "\n",
      "Train Epoch: 8 [3200/60000] Loss: 0.451012 Acc: 0.7812\n",
      "Train Epoch: 8 [6400/60000] Loss: 0.381469 Acc: 0.8125\n",
      "Train Epoch: 8 [9600/60000] Loss: 0.318145 Acc: 0.8750\n",
      "Train Epoch: 8 [12800/60000] Loss: 0.263837 Acc: 0.8750\n",
      "Train Epoch: 8 [16000/60000] Loss: 0.212179 Acc: 0.9062\n",
      "Train Epoch: 8 [19200/60000] Loss: 0.154739 Acc: 0.9375\n",
      "Train Epoch: 8 [22400/60000] Loss: 0.294370 Acc: 0.9062\n",
      "Train Epoch: 8 [25600/60000] Loss: 0.330363 Acc: 0.7812\n",
      "Train Epoch: 8 [28800/60000] Loss: 0.253140 Acc: 0.9062\n",
      "Train Epoch: 8 [32000/60000] Loss: 0.114473 Acc: 1.0000\n",
      "Train Epoch: 8 [35200/60000] Loss: 0.207704 Acc: 0.9375\n",
      "Train Epoch: 8 [38400/60000] Loss: 0.454932 Acc: 0.8750\n",
      "Train Epoch: 8 [41600/60000] Loss: 0.229577 Acc: 0.9062\n",
      "Train Epoch: 8 [44800/60000] Loss: 0.611968 Acc: 0.7188\n",
      "Train Epoch: 8 [48000/60000] Loss: 0.234056 Acc: 0.9062\n",
      "Train Epoch: 8 [51200/60000] Loss: 0.247160 Acc: 0.9062\n",
      "Train Epoch: 8 [54400/60000] Loss: 0.087116 Acc: 1.0000\n",
      "Train Epoch: 8 [57600/60000] Loss: 0.225548 Acc: 0.9375\n",
      "Elapsed 44.95s, 4.99 s/epoch, 0.00 s/batch, ets 54.94s\n",
      "\n",
      "Test set: Average loss: 0.3428, Accuracy: 8737/10000 (87%)\n",
      "\n",
      "Train Epoch: 9 [3200/60000] Loss: 0.329840 Acc: 0.9062\n",
      "Train Epoch: 9 [6400/60000] Loss: 0.215203 Acc: 0.9062\n",
      "Train Epoch: 9 [9600/60000] Loss: 0.222762 Acc: 0.9688\n",
      "Train Epoch: 9 [12800/60000] Loss: 0.209492 Acc: 0.9375\n",
      "Train Epoch: 9 [16000/60000] Loss: 0.318883 Acc: 0.9375\n",
      "Train Epoch: 9 [19200/60000] Loss: 0.282998 Acc: 0.9062\n",
      "Train Epoch: 9 [22400/60000] Loss: 0.335231 Acc: 0.9062\n",
      "Train Epoch: 9 [25600/60000] Loss: 0.403989 Acc: 0.8750\n",
      "Train Epoch: 9 [28800/60000] Loss: 0.362405 Acc: 0.8438\n",
      "Train Epoch: 9 [32000/60000] Loss: 0.314748 Acc: 0.8750\n",
      "Train Epoch: 9 [35200/60000] Loss: 0.430129 Acc: 0.8438\n",
      "Train Epoch: 9 [38400/60000] Loss: 0.268277 Acc: 0.9062\n",
      "Train Epoch: 9 [41600/60000] Loss: 0.355794 Acc: 0.9062\n",
      "Train Epoch: 9 [44800/60000] Loss: 0.593336 Acc: 0.8438\n",
      "Train Epoch: 9 [48000/60000] Loss: 0.344255 Acc: 0.9062\n",
      "Train Epoch: 9 [51200/60000] Loss: 0.366221 Acc: 0.8750\n",
      "Train Epoch: 9 [54400/60000] Loss: 0.272416 Acc: 0.9062\n",
      "Train Epoch: 9 [57600/60000] Loss: 0.260920 Acc: 0.9062\n",
      "Elapsed 50.01s, 5.00 s/epoch, 0.00 s/batch, ets 50.01s\n",
      "\n",
      "Test set: Average loss: 0.3487, Accuracy: 8707/10000 (87%)\n",
      "\n",
      "Train Epoch: 10 [3200/60000] Loss: 0.372444 Acc: 0.8750\n",
      "Train Epoch: 10 [6400/60000] Loss: 0.263277 Acc: 0.9375\n",
      "Train Epoch: 10 [9600/60000] Loss: 0.143046 Acc: 0.9375\n",
      "Train Epoch: 10 [12800/60000] Loss: 0.379254 Acc: 0.8750\n",
      "Train Epoch: 10 [16000/60000] Loss: 0.261596 Acc: 0.8438\n",
      "Train Epoch: 10 [19200/60000] Loss: 0.528848 Acc: 0.8750\n",
      "Train Epoch: 10 [22400/60000] Loss: 0.349504 Acc: 0.8750\n",
      "Train Epoch: 10 [25600/60000] Loss: 0.355360 Acc: 0.8438\n",
      "Train Epoch: 10 [28800/60000] Loss: 0.602511 Acc: 0.7812\n",
      "Train Epoch: 10 [32000/60000] Loss: 0.297217 Acc: 0.9062\n",
      "Train Epoch: 10 [35200/60000] Loss: 0.270590 Acc: 0.8438\n",
      "Train Epoch: 10 [38400/60000] Loss: 0.162405 Acc: 0.9688\n",
      "Train Epoch: 10 [41600/60000] Loss: 0.151229 Acc: 0.9688\n",
      "Train Epoch: 10 [44800/60000] Loss: 0.292059 Acc: 0.8125\n",
      "Train Epoch: 10 [48000/60000] Loss: 0.154931 Acc: 0.9375\n",
      "Train Epoch: 10 [51200/60000] Loss: 0.242059 Acc: 0.9062\n",
      "Train Epoch: 10 [54400/60000] Loss: 0.271407 Acc: 0.8438\n",
      "Train Epoch: 10 [57600/60000] Loss: 0.522424 Acc: 0.8750\n",
      "Elapsed 55.27s, 5.02 s/epoch, 0.00 s/batch, ets 45.22s\n",
      "\n",
      "Test set: Average loss: 0.3435, Accuracy: 8719/10000 (87%)\n",
      "\n",
      "Train Epoch: 11 [3200/60000] Loss: 0.189581 Acc: 0.9062\n",
      "Train Epoch: 11 [6400/60000] Loss: 0.395551 Acc: 0.8438\n",
      "Train Epoch: 11 [9600/60000] Loss: 0.232311 Acc: 0.9062\n",
      "Train Epoch: 11 [12800/60000] Loss: 0.443186 Acc: 0.8750\n",
      "Train Epoch: 11 [16000/60000] Loss: 0.312323 Acc: 0.8750\n",
      "Train Epoch: 11 [19200/60000] Loss: 0.284754 Acc: 0.8750\n",
      "Train Epoch: 11 [22400/60000] Loss: 0.161329 Acc: 0.9688\n",
      "Train Epoch: 11 [25600/60000] Loss: 0.114149 Acc: 0.9688\n",
      "Train Epoch: 11 [28800/60000] Loss: 0.695784 Acc: 0.8438\n",
      "Train Epoch: 11 [32000/60000] Loss: 0.152282 Acc: 0.9688\n",
      "Train Epoch: 11 [35200/60000] Loss: 0.160105 Acc: 0.9688\n",
      "Train Epoch: 11 [38400/60000] Loss: 0.336821 Acc: 0.9375\n",
      "Train Epoch: 11 [41600/60000] Loss: 0.210240 Acc: 0.9062\n",
      "Train Epoch: 11 [44800/60000] Loss: 0.248939 Acc: 0.9062\n",
      "Train Epoch: 11 [48000/60000] Loss: 0.311129 Acc: 0.8750\n",
      "Train Epoch: 11 [51200/60000] Loss: 0.155601 Acc: 0.9688\n",
      "Train Epoch: 11 [54400/60000] Loss: 0.132407 Acc: 1.0000\n",
      "Train Epoch: 11 [57600/60000] Loss: 0.357035 Acc: 0.8438\n",
      "Elapsed 60.93s, 5.08 s/epoch, 0.00 s/batch, ets 40.62s\n",
      "\n",
      "Test set: Average loss: 0.3356, Accuracy: 8749/10000 (87%)\n",
      "\n",
      "Train Epoch: 12 [3200/60000] Loss: 0.272962 Acc: 0.9062\n",
      "Train Epoch: 12 [6400/60000] Loss: 0.309574 Acc: 0.8750\n",
      "Train Epoch: 12 [9600/60000] Loss: 0.210046 Acc: 0.9375\n",
      "Train Epoch: 12 [12800/60000] Loss: 0.196122 Acc: 0.9375\n",
      "Train Epoch: 12 [16000/60000] Loss: 0.218525 Acc: 0.9062\n",
      "Train Epoch: 12 [19200/60000] Loss: 0.567525 Acc: 0.7812\n",
      "Train Epoch: 12 [22400/60000] Loss: 0.280749 Acc: 0.9062\n",
      "Train Epoch: 12 [25600/60000] Loss: 0.195420 Acc: 0.9062\n",
      "Train Epoch: 12 [28800/60000] Loss: 0.437388 Acc: 0.8750\n",
      "Train Epoch: 12 [32000/60000] Loss: 0.298768 Acc: 0.8750\n",
      "Train Epoch: 12 [35200/60000] Loss: 0.131134 Acc: 0.9688\n",
      "Train Epoch: 12 [38400/60000] Loss: 0.406746 Acc: 0.8438\n",
      "Train Epoch: 12 [41600/60000] Loss: 0.205553 Acc: 0.9688\n",
      "Train Epoch: 12 [44800/60000] Loss: 0.123795 Acc: 0.9375\n",
      "Train Epoch: 12 [48000/60000] Loss: 0.080074 Acc: 1.0000\n",
      "Train Epoch: 12 [51200/60000] Loss: 0.313889 Acc: 0.9062\n",
      "Train Epoch: 12 [54400/60000] Loss: 0.512063 Acc: 0.8125\n",
      "Train Epoch: 12 [57600/60000] Loss: 0.689926 Acc: 0.8125\n",
      "Elapsed 66.06s, 5.08 s/epoch, 0.00 s/batch, ets 35.57s\n",
      "\n",
      "Test set: Average loss: 0.3133, Accuracy: 8854/10000 (89%)\n",
      "\n",
      "Train Epoch: 13 [3200/60000] Loss: 0.250622 Acc: 0.9062\n",
      "Train Epoch: 13 [6400/60000] Loss: 0.275021 Acc: 0.8750\n",
      "Train Epoch: 13 [9600/60000] Loss: 0.198929 Acc: 0.9062\n",
      "Train Epoch: 13 [12800/60000] Loss: 0.270229 Acc: 0.9062\n",
      "Train Epoch: 13 [16000/60000] Loss: 0.189386 Acc: 0.9062\n",
      "Train Epoch: 13 [19200/60000] Loss: 0.323165 Acc: 0.8438\n",
      "Train Epoch: 13 [22400/60000] Loss: 0.197467 Acc: 0.9375\n",
      "Train Epoch: 13 [25600/60000] Loss: 0.423692 Acc: 0.7812\n",
      "Train Epoch: 13 [28800/60000] Loss: 0.226826 Acc: 0.9062\n",
      "Train Epoch: 13 [32000/60000] Loss: 0.208213 Acc: 0.9062\n",
      "Train Epoch: 13 [35200/60000] Loss: 0.431862 Acc: 0.7500\n",
      "Train Epoch: 13 [38400/60000] Loss: 0.208994 Acc: 0.9062\n",
      "Train Epoch: 13 [41600/60000] Loss: 0.292644 Acc: 0.8438\n",
      "Train Epoch: 13 [44800/60000] Loss: 0.040032 Acc: 1.0000\n",
      "Train Epoch: 13 [48000/60000] Loss: 0.257461 Acc: 0.8750\n",
      "Train Epoch: 13 [51200/60000] Loss: 0.416021 Acc: 0.8438\n",
      "Train Epoch: 13 [54400/60000] Loss: 0.385775 Acc: 0.8438\n",
      "Train Epoch: 13 [57600/60000] Loss: 0.172355 Acc: 0.9375\n",
      "Elapsed 71.03s, 5.07 s/epoch, 0.00 s/batch, ets 30.44s\n",
      "\n",
      "Test set: Average loss: 0.3192, Accuracy: 8841/10000 (88%)\n",
      "\n",
      "Train Epoch: 14 [3200/60000] Loss: 0.191240 Acc: 0.9062\n",
      "Train Epoch: 14 [6400/60000] Loss: 0.398377 Acc: 0.8750\n",
      "Train Epoch: 14 [9600/60000] Loss: 0.316518 Acc: 0.9062\n",
      "Train Epoch: 14 [12800/60000] Loss: 0.222616 Acc: 0.9688\n",
      "Train Epoch: 14 [16000/60000] Loss: 0.280649 Acc: 0.8438\n",
      "Train Epoch: 14 [19200/60000] Loss: 0.157305 Acc: 0.9375\n",
      "Train Epoch: 14 [22400/60000] Loss: 0.656377 Acc: 0.9062\n",
      "Train Epoch: 14 [25600/60000] Loss: 0.277992 Acc: 0.9375\n",
      "Train Epoch: 14 [28800/60000] Loss: 0.496198 Acc: 0.8125\n",
      "Train Epoch: 14 [32000/60000] Loss: 0.162124 Acc: 0.9375\n",
      "Train Epoch: 14 [35200/60000] Loss: 0.305521 Acc: 0.9062\n",
      "Train Epoch: 14 [38400/60000] Loss: 0.247914 Acc: 0.9688\n",
      "Train Epoch: 14 [41600/60000] Loss: 0.166582 Acc: 0.9375\n",
      "Train Epoch: 14 [44800/60000] Loss: 0.339494 Acc: 0.9375\n",
      "Train Epoch: 14 [48000/60000] Loss: 0.251293 Acc: 0.9062\n",
      "Train Epoch: 14 [51200/60000] Loss: 0.229311 Acc: 0.9375\n",
      "Train Epoch: 14 [54400/60000] Loss: 0.216963 Acc: 0.9062\n",
      "Train Epoch: 14 [57600/60000] Loss: 0.221197 Acc: 0.9062\n",
      "Elapsed 77.10s, 5.14 s/epoch, 0.00 s/batch, ets 25.70s\n",
      "\n",
      "Test set: Average loss: 0.3206, Accuracy: 8825/10000 (88%)\n",
      "\n",
      "Train Epoch: 15 [3200/60000] Loss: 0.347357 Acc: 0.8438\n",
      "Train Epoch: 15 [6400/60000] Loss: 0.180465 Acc: 0.9375\n",
      "Train Epoch: 15 [9600/60000] Loss: 0.089281 Acc: 0.9375\n",
      "Train Epoch: 15 [12800/60000] Loss: 0.295233 Acc: 0.9062\n",
      "Train Epoch: 15 [16000/60000] Loss: 0.228122 Acc: 0.9375\n",
      "Train Epoch: 15 [19200/60000] Loss: 0.354298 Acc: 0.7812\n",
      "Train Epoch: 15 [22400/60000] Loss: 0.448340 Acc: 0.8750\n",
      "Train Epoch: 15 [25600/60000] Loss: 0.268669 Acc: 0.8750\n",
      "Train Epoch: 15 [28800/60000] Loss: 0.348862 Acc: 0.9062\n",
      "Train Epoch: 15 [32000/60000] Loss: 0.248276 Acc: 0.8750\n",
      "Train Epoch: 15 [35200/60000] Loss: 0.280256 Acc: 0.8750\n",
      "Train Epoch: 15 [38400/60000] Loss: 0.207520 Acc: 0.9062\n",
      "Train Epoch: 15 [41600/60000] Loss: 0.380887 Acc: 0.9062\n",
      "Train Epoch: 15 [44800/60000] Loss: 0.444585 Acc: 0.8438\n",
      "Train Epoch: 15 [48000/60000] Loss: 0.446952 Acc: 0.8125\n",
      "Train Epoch: 15 [51200/60000] Loss: 0.194733 Acc: 0.8750\n",
      "Train Epoch: 15 [54400/60000] Loss: 0.165928 Acc: 0.9375\n",
      "Train Epoch: 15 [57600/60000] Loss: 0.257107 Acc: 0.8750\n",
      "Elapsed 82.20s, 5.14 s/epoch, 0.00 s/batch, ets 20.55s\n",
      "\n",
      "Test set: Average loss: 0.3183, Accuracy: 8854/10000 (89%)\n",
      "\n",
      "Train Epoch: 16 [3200/60000] Loss: 0.173258 Acc: 0.8750\n",
      "Train Epoch: 16 [6400/60000] Loss: 0.161835 Acc: 0.9375\n",
      "Train Epoch: 16 [9600/60000] Loss: 0.270013 Acc: 0.9062\n",
      "Train Epoch: 16 [12800/60000] Loss: 0.118912 Acc: 0.9688\n",
      "Train Epoch: 16 [16000/60000] Loss: 0.274005 Acc: 0.9375\n",
      "Train Epoch: 16 [19200/60000] Loss: 0.269689 Acc: 0.8750\n",
      "Train Epoch: 16 [22400/60000] Loss: 0.211426 Acc: 0.9062\n",
      "Train Epoch: 16 [25600/60000] Loss: 0.403352 Acc: 0.8125\n",
      "Train Epoch: 16 [28800/60000] Loss: 0.434489 Acc: 0.8750\n",
      "Train Epoch: 16 [32000/60000] Loss: 0.236402 Acc: 0.8750\n",
      "Train Epoch: 16 [35200/60000] Loss: 0.162197 Acc: 0.9688\n",
      "Train Epoch: 16 [38400/60000] Loss: 0.380284 Acc: 0.8750\n",
      "Train Epoch: 16 [41600/60000] Loss: 0.203365 Acc: 0.9375\n",
      "Train Epoch: 16 [44800/60000] Loss: 0.450166 Acc: 0.8750\n",
      "Train Epoch: 16 [48000/60000] Loss: 0.225920 Acc: 0.9062\n",
      "Train Epoch: 16 [51200/60000] Loss: 0.050707 Acc: 0.9688\n",
      "Train Epoch: 16 [54400/60000] Loss: 0.195247 Acc: 0.9688\n",
      "Train Epoch: 16 [57600/60000] Loss: 0.130533 Acc: 0.9375\n",
      "Elapsed 87.50s, 5.15 s/epoch, 0.00 s/batch, ets 15.44s\n",
      "\n",
      "Test set: Average loss: 0.3213, Accuracy: 8845/10000 (88%)\n",
      "\n",
      "Train Epoch: 17 [3200/60000] Loss: 0.303715 Acc: 0.8750\n",
      "Train Epoch: 17 [6400/60000] Loss: 0.181871 Acc: 0.9688\n",
      "Train Epoch: 17 [9600/60000] Loss: 0.407309 Acc: 0.9062\n",
      "Train Epoch: 17 [12800/60000] Loss: 0.174545 Acc: 0.9062\n",
      "Train Epoch: 17 [16000/60000] Loss: 0.164975 Acc: 0.9688\n",
      "Train Epoch: 17 [19200/60000] Loss: 0.178184 Acc: 0.9375\n",
      "Train Epoch: 17 [22400/60000] Loss: 0.183132 Acc: 0.9062\n",
      "Train Epoch: 17 [25600/60000] Loss: 0.209683 Acc: 0.8438\n",
      "Train Epoch: 17 [28800/60000] Loss: 0.415854 Acc: 0.8438\n",
      "Train Epoch: 17 [32000/60000] Loss: 0.259162 Acc: 0.9375\n",
      "Train Epoch: 17 [35200/60000] Loss: 0.206039 Acc: 0.8750\n",
      "Train Epoch: 17 [38400/60000] Loss: 0.206489 Acc: 0.8750\n",
      "Train Epoch: 17 [41600/60000] Loss: 0.173920 Acc: 0.9375\n",
      "Train Epoch: 17 [44800/60000] Loss: 0.423522 Acc: 0.8750\n",
      "Train Epoch: 17 [48000/60000] Loss: 0.374585 Acc: 0.8750\n",
      "Train Epoch: 17 [51200/60000] Loss: 0.231912 Acc: 0.9062\n",
      "Train Epoch: 17 [54400/60000] Loss: 0.210544 Acc: 0.8750\n",
      "Train Epoch: 17 [57600/60000] Loss: 0.344768 Acc: 0.8750\n",
      "Elapsed 92.56s, 5.14 s/epoch, 0.00 s/batch, ets 10.28s\n",
      "\n",
      "Test set: Average loss: 0.3200, Accuracy: 8844/10000 (88%)\n",
      "\n",
      "Train Epoch: 18 [3200/60000] Loss: 0.457895 Acc: 0.8438\n",
      "Train Epoch: 18 [6400/60000] Loss: 0.157698 Acc: 0.9375\n",
      "Train Epoch: 18 [9600/60000] Loss: 0.223204 Acc: 0.9375\n",
      "Train Epoch: 18 [12800/60000] Loss: 0.089918 Acc: 0.9688\n",
      "Train Epoch: 18 [16000/60000] Loss: 0.164539 Acc: 0.9375\n",
      "Train Epoch: 18 [19200/60000] Loss: 0.205649 Acc: 0.9375\n",
      "Train Epoch: 18 [22400/60000] Loss: 0.187149 Acc: 0.9375\n",
      "Train Epoch: 18 [25600/60000] Loss: 0.408196 Acc: 0.8438\n",
      "Train Epoch: 18 [28800/60000] Loss: 0.163816 Acc: 0.9688\n",
      "Train Epoch: 18 [32000/60000] Loss: 0.183522 Acc: 0.9375\n",
      "Train Epoch: 18 [35200/60000] Loss: 0.333781 Acc: 0.9062\n",
      "Train Epoch: 18 [38400/60000] Loss: 0.117161 Acc: 0.9688\n",
      "Train Epoch: 18 [41600/60000] Loss: 0.284760 Acc: 0.9062\n",
      "Train Epoch: 18 [44800/60000] Loss: 0.230987 Acc: 0.8750\n",
      "Train Epoch: 18 [48000/60000] Loss: 0.102499 Acc: 0.9688\n",
      "Train Epoch: 18 [51200/60000] Loss: 0.315979 Acc: 0.9375\n",
      "Train Epoch: 18 [54400/60000] Loss: 0.326697 Acc: 0.9062\n",
      "Train Epoch: 18 [57600/60000] Loss: 0.300227 Acc: 0.9062\n",
      "Elapsed 97.94s, 5.15 s/epoch, 0.00 s/batch, ets 5.15s\n",
      "\n",
      "Test set: Average loss: 0.3020, Accuracy: 8870/10000 (89%)\n",
      "\n",
      "Train Epoch: 19 [3200/60000] Loss: 0.331718 Acc: 0.9062\n",
      "Train Epoch: 19 [6400/60000] Loss: 0.190397 Acc: 0.9062\n",
      "Train Epoch: 19 [9600/60000] Loss: 0.249660 Acc: 0.9062\n",
      "Train Epoch: 19 [12800/60000] Loss: 0.244395 Acc: 0.9062\n",
      "Train Epoch: 19 [16000/60000] Loss: 0.183808 Acc: 0.9688\n",
      "Train Epoch: 19 [19200/60000] Loss: 0.237845 Acc: 0.9062\n",
      "Train Epoch: 19 [22400/60000] Loss: 0.070546 Acc: 1.0000\n",
      "Train Epoch: 19 [25600/60000] Loss: 0.290309 Acc: 0.8438\n",
      "Train Epoch: 19 [28800/60000] Loss: 0.137843 Acc: 0.9375\n",
      "Train Epoch: 19 [32000/60000] Loss: 0.241275 Acc: 0.9062\n",
      "Train Epoch: 19 [35200/60000] Loss: 0.233651 Acc: 0.9062\n",
      "Train Epoch: 19 [38400/60000] Loss: 0.223621 Acc: 0.8750\n",
      "Train Epoch: 19 [41600/60000] Loss: 0.199526 Acc: 0.9375\n",
      "Train Epoch: 19 [44800/60000] Loss: 0.550081 Acc: 0.8750\n",
      "Train Epoch: 19 [48000/60000] Loss: 0.115865 Acc: 0.9375\n",
      "Train Epoch: 19 [51200/60000] Loss: 0.137282 Acc: 0.9688\n",
      "Train Epoch: 19 [54400/60000] Loss: 0.284986 Acc: 0.8750\n",
      "Train Epoch: 19 [57600/60000] Loss: 0.342477 Acc: 0.8125\n",
      "Elapsed 103.94s, 5.20 s/epoch, 0.00 s/batch, ets 0.00s\n",
      "\n",
      "Test set: Average loss: 0.3165, Accuracy: 8828/10000 (88%)\n",
      "\n",
      "Total time: 104.62, Best Loss: 0.302\n",
      "Train Epoch: 0 [3200/60000] Loss: 1.800828 Acc: 0.6250\n",
      "Train Epoch: 0 [6400/60000] Loss: 1.062015 Acc: 0.6562\n",
      "Train Epoch: 0 [9600/60000] Loss: 1.081333 Acc: 0.6875\n",
      "Train Epoch: 0 [12800/60000] Loss: 0.746155 Acc: 0.6250\n",
      "Train Epoch: 0 [16000/60000] Loss: 0.751642 Acc: 0.7500\n",
      "Train Epoch: 0 [19200/60000] Loss: 0.589586 Acc: 0.7500\n",
      "Train Epoch: 0 [22400/60000] Loss: 0.546565 Acc: 0.8125\n",
      "Train Epoch: 0 [25600/60000] Loss: 0.640061 Acc: 0.8125\n",
      "Train Epoch: 0 [28800/60000] Loss: 0.518718 Acc: 0.8125\n",
      "Train Epoch: 0 [32000/60000] Loss: 0.427641 Acc: 0.8125\n",
      "Train Epoch: 0 [35200/60000] Loss: 0.643599 Acc: 0.7188\n",
      "Train Epoch: 0 [38400/60000] Loss: 0.454640 Acc: 0.8125\n",
      "Train Epoch: 0 [41600/60000] Loss: 0.447895 Acc: 0.8438\n",
      "Train Epoch: 0 [44800/60000] Loss: 0.552270 Acc: 0.8125\n",
      "Train Epoch: 0 [48000/60000] Loss: 0.650316 Acc: 0.7500\n",
      "Train Epoch: 0 [51200/60000] Loss: 0.378774 Acc: 0.8438\n",
      "Train Epoch: 0 [54400/60000] Loss: 0.510266 Acc: 0.7812\n",
      "Train Epoch: 0 [57600/60000] Loss: 0.455995 Acc: 0.8438\n",
      "Elapsed 4.75s, 4.75 s/epoch, 0.00 s/batch, ets 90.32s\n",
      "\n",
      "Test set: Average loss: 0.4552, Accuracy: 8392/10000 (84%)\n",
      "\n",
      "Train Epoch: 1 [3200/60000] Loss: 0.633431 Acc: 0.7188\n",
      "Train Epoch: 1 [6400/60000] Loss: 0.557615 Acc: 0.8125\n",
      "Train Epoch: 1 [9600/60000] Loss: 0.492640 Acc: 0.7500\n",
      "Train Epoch: 1 [12800/60000] Loss: 0.625949 Acc: 0.7500\n",
      "Train Epoch: 1 [16000/60000] Loss: 0.324963 Acc: 0.9062\n",
      "Train Epoch: 1 [19200/60000] Loss: 0.865817 Acc: 0.7188\n",
      "Train Epoch: 1 [22400/60000] Loss: 0.311470 Acc: 0.8438\n",
      "Train Epoch: 1 [25600/60000] Loss: 0.369526 Acc: 0.8438\n",
      "Train Epoch: 1 [28800/60000] Loss: 0.346205 Acc: 0.8125\n",
      "Train Epoch: 1 [32000/60000] Loss: 0.327687 Acc: 0.9062\n",
      "Train Epoch: 1 [35200/60000] Loss: 0.204598 Acc: 0.9688\n",
      "Train Epoch: 1 [38400/60000] Loss: 0.615550 Acc: 0.7812\n",
      "Train Epoch: 1 [41600/60000] Loss: 0.289185 Acc: 0.9062\n",
      "Train Epoch: 1 [44800/60000] Loss: 0.260047 Acc: 0.8750\n",
      "Train Epoch: 1 [48000/60000] Loss: 0.480290 Acc: 0.8125\n",
      "Train Epoch: 1 [51200/60000] Loss: 0.260279 Acc: 0.8750\n",
      "Train Epoch: 1 [54400/60000] Loss: 0.632766 Acc: 0.7188\n",
      "Train Epoch: 1 [57600/60000] Loss: 0.443857 Acc: 0.8125\n",
      "Elapsed 10.47s, 5.23 s/epoch, 0.00 s/batch, ets 94.22s\n",
      "\n",
      "Test set: Average loss: 0.3903, Accuracy: 8610/10000 (86%)\n",
      "\n",
      "Train Epoch: 2 [3200/60000] Loss: 0.478921 Acc: 0.8125\n",
      "Train Epoch: 2 [6400/60000] Loss: 0.384711 Acc: 0.8750\n",
      "Train Epoch: 2 [9600/60000] Loss: 0.138985 Acc: 1.0000\n",
      "Train Epoch: 2 [12800/60000] Loss: 0.417335 Acc: 0.8750\n",
      "Train Epoch: 2 [16000/60000] Loss: 0.231586 Acc: 0.8438\n",
      "Train Epoch: 2 [19200/60000] Loss: 0.577315 Acc: 0.7812\n",
      "Train Epoch: 2 [22400/60000] Loss: 0.345670 Acc: 0.8750\n",
      "Train Epoch: 2 [25600/60000] Loss: 0.277443 Acc: 0.9375\n",
      "Train Epoch: 2 [28800/60000] Loss: 0.330824 Acc: 0.9062\n",
      "Train Epoch: 2 [32000/60000] Loss: 0.163937 Acc: 0.9688\n",
      "Train Epoch: 2 [35200/60000] Loss: 0.380589 Acc: 0.8125\n",
      "Train Epoch: 2 [38400/60000] Loss: 0.326299 Acc: 0.8750\n",
      "Train Epoch: 2 [41600/60000] Loss: 0.360516 Acc: 0.9375\n",
      "Train Epoch: 2 [44800/60000] Loss: 0.337612 Acc: 0.8750\n",
      "Train Epoch: 2 [48000/60000] Loss: 0.169939 Acc: 0.9688\n",
      "Train Epoch: 2 [51200/60000] Loss: 0.230255 Acc: 0.9688\n",
      "Train Epoch: 2 [54400/60000] Loss: 0.232974 Acc: 0.9062\n",
      "Train Epoch: 2 [57600/60000] Loss: 0.237852 Acc: 0.8750\n",
      "Elapsed 16.19s, 5.40 s/epoch, 0.00 s/batch, ets 91.74s\n",
      "\n",
      "Test set: Average loss: 0.3804, Accuracy: 8619/10000 (86%)\n",
      "\n",
      "Train Epoch: 3 [3200/60000] Loss: 0.230047 Acc: 0.9375\n",
      "Train Epoch: 3 [6400/60000] Loss: 0.291859 Acc: 0.9062\n",
      "Train Epoch: 3 [9600/60000] Loss: 0.582715 Acc: 0.7812\n",
      "Train Epoch: 3 [12800/60000] Loss: 0.340144 Acc: 0.9375\n",
      "Train Epoch: 3 [16000/60000] Loss: 0.250693 Acc: 0.9062\n",
      "Train Epoch: 3 [19200/60000] Loss: 0.358073 Acc: 0.8125\n",
      "Train Epoch: 3 [22400/60000] Loss: 0.473652 Acc: 0.8750\n",
      "Train Epoch: 3 [25600/60000] Loss: 0.333944 Acc: 0.8438\n",
      "Train Epoch: 3 [28800/60000] Loss: 0.306063 Acc: 0.9062\n",
      "Train Epoch: 3 [32000/60000] Loss: 0.222904 Acc: 0.8750\n",
      "Train Epoch: 3 [35200/60000] Loss: 0.358741 Acc: 0.8750\n",
      "Train Epoch: 3 [38400/60000] Loss: 0.216337 Acc: 0.9062\n",
      "Train Epoch: 3 [41600/60000] Loss: 0.390649 Acc: 0.8750\n",
      "Train Epoch: 3 [44800/60000] Loss: 0.237377 Acc: 0.9062\n",
      "Train Epoch: 3 [48000/60000] Loss: 0.804101 Acc: 0.8125\n",
      "Train Epoch: 3 [51200/60000] Loss: 0.677060 Acc: 0.8125\n",
      "Train Epoch: 3 [54400/60000] Loss: 0.475500 Acc: 0.8125\n",
      "Train Epoch: 3 [57600/60000] Loss: 0.338546 Acc: 0.9062\n",
      "Elapsed 21.45s, 5.36 s/epoch, 0.00 s/batch, ets 85.79s\n",
      "\n",
      "Test set: Average loss: 0.3367, Accuracy: 8781/10000 (88%)\n",
      "\n",
      "Train Epoch: 4 [3200/60000] Loss: 0.237890 Acc: 0.9062\n",
      "Train Epoch: 4 [6400/60000] Loss: 0.253772 Acc: 0.9375\n",
      "Train Epoch: 4 [9600/60000] Loss: 0.292110 Acc: 0.8750\n",
      "Train Epoch: 4 [12800/60000] Loss: 0.189371 Acc: 0.9062\n",
      "Train Epoch: 4 [16000/60000] Loss: 0.120499 Acc: 0.9688\n",
      "Train Epoch: 4 [19200/60000] Loss: 0.322811 Acc: 0.8438\n",
      "Train Epoch: 4 [22400/60000] Loss: 0.257294 Acc: 0.9062\n",
      "Train Epoch: 4 [25600/60000] Loss: 0.269993 Acc: 0.9062\n",
      "Train Epoch: 4 [28800/60000] Loss: 0.454844 Acc: 0.8750\n",
      "Train Epoch: 4 [32000/60000] Loss: 0.312681 Acc: 0.8750\n",
      "Train Epoch: 4 [35200/60000] Loss: 0.334609 Acc: 0.8750\n",
      "Train Epoch: 4 [38400/60000] Loss: 0.234997 Acc: 0.9062\n",
      "Train Epoch: 4 [41600/60000] Loss: 0.392794 Acc: 0.8438\n",
      "Train Epoch: 4 [44800/60000] Loss: 0.149037 Acc: 1.0000\n",
      "Train Epoch: 4 [48000/60000] Loss: 0.165581 Acc: 1.0000\n",
      "Train Epoch: 4 [51200/60000] Loss: 0.235694 Acc: 0.9062\n",
      "Train Epoch: 4 [54400/60000] Loss: 0.272268 Acc: 0.9062\n",
      "Train Epoch: 4 [57600/60000] Loss: 0.357557 Acc: 0.8438\n",
      "Elapsed 27.22s, 5.44 s/epoch, 0.00 s/batch, ets 81.65s\n",
      "\n",
      "Test set: Average loss: 0.3551, Accuracy: 8727/10000 (87%)\n",
      "\n",
      "Train Epoch: 5 [3200/60000] Loss: 0.345867 Acc: 0.8750\n",
      "Train Epoch: 5 [6400/60000] Loss: 0.262541 Acc: 0.9375\n",
      "Train Epoch: 5 [9600/60000] Loss: 0.290375 Acc: 0.9375\n",
      "Train Epoch: 5 [12800/60000] Loss: 0.383720 Acc: 0.9062\n",
      "Train Epoch: 5 [16000/60000] Loss: 0.358262 Acc: 0.8438\n",
      "Train Epoch: 5 [19200/60000] Loss: 0.360782 Acc: 0.8438\n",
      "Train Epoch: 5 [22400/60000] Loss: 0.231734 Acc: 0.9062\n",
      "Train Epoch: 5 [25600/60000] Loss: 0.073956 Acc: 1.0000\n",
      "Train Epoch: 5 [28800/60000] Loss: 0.336875 Acc: 0.8125\n",
      "Train Epoch: 5 [32000/60000] Loss: 0.272547 Acc: 0.9062\n",
      "Train Epoch: 5 [35200/60000] Loss: 0.118252 Acc: 0.9375\n",
      "Train Epoch: 5 [38400/60000] Loss: 0.391645 Acc: 0.8750\n",
      "Train Epoch: 5 [41600/60000] Loss: 0.197054 Acc: 0.9062\n",
      "Train Epoch: 5 [44800/60000] Loss: 0.225153 Acc: 0.9375\n",
      "Train Epoch: 5 [48000/60000] Loss: 0.521230 Acc: 0.7812\n",
      "Train Epoch: 5 [51200/60000] Loss: 0.232475 Acc: 0.9062\n",
      "Train Epoch: 5 [54400/60000] Loss: 0.209355 Acc: 0.9062\n",
      "Train Epoch: 5 [57600/60000] Loss: 0.181089 Acc: 0.9688\n",
      "Elapsed 32.92s, 5.49 s/epoch, 0.00 s/batch, ets 76.82s\n",
      "\n",
      "Test set: Average loss: 0.3388, Accuracy: 8774/10000 (88%)\n",
      "\n",
      "Train Epoch: 6 [3200/60000] Loss: 0.088293 Acc: 0.9688\n",
      "Train Epoch: 6 [6400/60000] Loss: 0.285472 Acc: 0.9062\n",
      "Train Epoch: 6 [9600/60000] Loss: 0.253070 Acc: 0.8125\n",
      "Train Epoch: 6 [12800/60000] Loss: 0.112801 Acc: 1.0000\n",
      "Train Epoch: 6 [16000/60000] Loss: 0.188253 Acc: 0.9062\n",
      "Train Epoch: 6 [19200/60000] Loss: 0.161477 Acc: 0.9688\n",
      "Train Epoch: 6 [22400/60000] Loss: 0.277028 Acc: 0.9062\n",
      "Train Epoch: 6 [25600/60000] Loss: 0.312763 Acc: 0.8750\n",
      "Train Epoch: 6 [28800/60000] Loss: 0.278672 Acc: 0.8750\n",
      "Train Epoch: 6 [32000/60000] Loss: 0.199421 Acc: 0.9062\n",
      "Train Epoch: 6 [35200/60000] Loss: 0.268676 Acc: 0.8750\n",
      "Train Epoch: 6 [38400/60000] Loss: 0.439957 Acc: 0.8125\n",
      "Train Epoch: 6 [41600/60000] Loss: 0.138767 Acc: 0.9688\n",
      "Train Epoch: 6 [44800/60000] Loss: 0.433020 Acc: 0.8750\n",
      "Train Epoch: 6 [48000/60000] Loss: 0.197202 Acc: 0.9688\n",
      "Train Epoch: 6 [51200/60000] Loss: 0.298070 Acc: 0.8438\n",
      "Train Epoch: 6 [54400/60000] Loss: 0.291146 Acc: 0.9062\n",
      "Train Epoch: 6 [57600/60000] Loss: 0.327368 Acc: 0.8750\n",
      "Elapsed 38.83s, 5.55 s/epoch, 0.00 s/batch, ets 72.12s\n",
      "\n",
      "Test set: Average loss: 0.3238, Accuracy: 8832/10000 (88%)\n",
      "\n",
      "Train Epoch: 7 [3200/60000] Loss: 0.284898 Acc: 0.8750\n",
      "Train Epoch: 7 [6400/60000] Loss: 0.585445 Acc: 0.8125\n",
      "Train Epoch: 7 [9600/60000] Loss: 0.244137 Acc: 0.8750\n",
      "Train Epoch: 7 [12800/60000] Loss: 0.188698 Acc: 0.9062\n",
      "Train Epoch: 7 [16000/60000] Loss: 0.253894 Acc: 0.9062\n",
      "Train Epoch: 7 [19200/60000] Loss: 0.395810 Acc: 0.9062\n",
      "Train Epoch: 7 [22400/60000] Loss: 0.279278 Acc: 0.9062\n",
      "Train Epoch: 7 [25600/60000] Loss: 0.315408 Acc: 0.9375\n",
      "Train Epoch: 7 [28800/60000] Loss: 0.266529 Acc: 0.9375\n",
      "Train Epoch: 7 [32000/60000] Loss: 0.361390 Acc: 0.8750\n",
      "Train Epoch: 7 [35200/60000] Loss: 0.387590 Acc: 0.8750\n",
      "Train Epoch: 7 [38400/60000] Loss: 0.482894 Acc: 0.8125\n",
      "Train Epoch: 7 [41600/60000] Loss: 0.173500 Acc: 0.9375\n",
      "Train Epoch: 7 [44800/60000] Loss: 0.169232 Acc: 0.9062\n",
      "Train Epoch: 7 [48000/60000] Loss: 0.158131 Acc: 0.9062\n",
      "Train Epoch: 7 [51200/60000] Loss: 0.130912 Acc: 0.9688\n",
      "Train Epoch: 7 [54400/60000] Loss: 0.249729 Acc: 0.9062\n",
      "Train Epoch: 7 [57600/60000] Loss: 0.196216 Acc: 0.9062\n",
      "Elapsed 44.85s, 5.61 s/epoch, 0.00 s/batch, ets 67.27s\n",
      "\n",
      "Test set: Average loss: 0.3083, Accuracy: 8905/10000 (89%)\n",
      "\n",
      "Train Epoch: 8 [3200/60000] Loss: 0.348590 Acc: 0.8750\n",
      "Train Epoch: 8 [6400/60000] Loss: 0.313870 Acc: 0.8438\n",
      "Train Epoch: 8 [9600/60000] Loss: 0.337646 Acc: 0.8125\n",
      "Train Epoch: 8 [12800/60000] Loss: 0.193869 Acc: 0.9375\n",
      "Train Epoch: 8 [16000/60000] Loss: 0.155271 Acc: 0.9688\n",
      "Train Epoch: 8 [19200/60000] Loss: 0.154563 Acc: 0.9062\n",
      "Train Epoch: 8 [22400/60000] Loss: 0.229320 Acc: 0.8750\n",
      "Train Epoch: 8 [25600/60000] Loss: 0.363438 Acc: 0.8438\n",
      "Train Epoch: 8 [28800/60000] Loss: 0.111412 Acc: 0.9688\n",
      "Train Epoch: 8 [32000/60000] Loss: 0.078841 Acc: 0.9688\n",
      "Train Epoch: 8 [35200/60000] Loss: 0.321017 Acc: 0.9062\n",
      "Train Epoch: 8 [38400/60000] Loss: 0.476483 Acc: 0.8125\n",
      "Train Epoch: 8 [41600/60000] Loss: 0.170127 Acc: 0.9375\n",
      "Train Epoch: 8 [44800/60000] Loss: 0.436420 Acc: 0.7500\n",
      "Train Epoch: 8 [48000/60000] Loss: 0.270378 Acc: 0.8750\n",
      "Train Epoch: 8 [51200/60000] Loss: 0.244438 Acc: 0.8750\n",
      "Train Epoch: 8 [54400/60000] Loss: 0.055638 Acc: 1.0000\n",
      "Train Epoch: 8 [57600/60000] Loss: 0.157978 Acc: 0.9375\n",
      "Elapsed 50.78s, 5.64 s/epoch, 0.00 s/batch, ets 62.06s\n",
      "\n",
      "Test set: Average loss: 0.2967, Accuracy: 8956/10000 (90%)\n",
      "\n",
      "Train Epoch: 9 [3200/60000] Loss: 0.229293 Acc: 0.9375\n",
      "Train Epoch: 9 [6400/60000] Loss: 0.114437 Acc: 0.9375\n",
      "Train Epoch: 9 [9600/60000] Loss: 0.175962 Acc: 0.9375\n",
      "Train Epoch: 9 [12800/60000] Loss: 0.123162 Acc: 0.9375\n",
      "Train Epoch: 9 [16000/60000] Loss: 0.209428 Acc: 0.9062\n",
      "Train Epoch: 9 [19200/60000] Loss: 0.182686 Acc: 0.9062\n",
      "Train Epoch: 9 [22400/60000] Loss: 0.217275 Acc: 0.9688\n",
      "Train Epoch: 9 [25600/60000] Loss: 0.248914 Acc: 0.9375\n",
      "Train Epoch: 9 [28800/60000] Loss: 0.303452 Acc: 0.9375\n",
      "Train Epoch: 9 [32000/60000] Loss: 0.266937 Acc: 0.9062\n",
      "Train Epoch: 9 [35200/60000] Loss: 0.237545 Acc: 0.9062\n",
      "Train Epoch: 9 [38400/60000] Loss: 0.211663 Acc: 0.9375\n",
      "Train Epoch: 9 [41600/60000] Loss: 0.279607 Acc: 0.9062\n",
      "Train Epoch: 9 [44800/60000] Loss: 0.400607 Acc: 0.8750\n",
      "Train Epoch: 9 [48000/60000] Loss: 0.400113 Acc: 0.8750\n",
      "Train Epoch: 9 [51200/60000] Loss: 0.339004 Acc: 0.9062\n",
      "Train Epoch: 9 [54400/60000] Loss: 0.164104 Acc: 0.9375\n",
      "Train Epoch: 9 [57600/60000] Loss: 0.126456 Acc: 0.9688\n",
      "Elapsed 56.76s, 5.68 s/epoch, 0.00 s/batch, ets 56.76s\n",
      "\n",
      "Test set: Average loss: 0.3260, Accuracy: 8834/10000 (88%)\n",
      "\n",
      "Train Epoch: 10 [3200/60000] Loss: 0.363307 Acc: 0.9375\n",
      "Train Epoch: 10 [6400/60000] Loss: 0.325264 Acc: 0.9375\n",
      "Train Epoch: 10 [9600/60000] Loss: 0.102491 Acc: 0.9688\n",
      "Train Epoch: 10 [12800/60000] Loss: 0.237391 Acc: 0.8750\n",
      "Train Epoch: 10 [16000/60000] Loss: 0.125385 Acc: 0.9688\n",
      "Train Epoch: 10 [19200/60000] Loss: 0.478131 Acc: 0.9375\n",
      "Train Epoch: 10 [22400/60000] Loss: 0.282240 Acc: 0.9375\n",
      "Train Epoch: 10 [25600/60000] Loss: 0.309928 Acc: 0.9062\n",
      "Train Epoch: 10 [28800/60000] Loss: 0.317682 Acc: 0.8750\n",
      "Train Epoch: 10 [32000/60000] Loss: 0.240321 Acc: 0.9375\n",
      "Train Epoch: 10 [35200/60000] Loss: 0.116923 Acc: 0.9688\n",
      "Train Epoch: 10 [38400/60000] Loss: 0.061510 Acc: 1.0000\n",
      "Train Epoch: 10 [41600/60000] Loss: 0.133253 Acc: 0.9688\n",
      "Train Epoch: 10 [44800/60000] Loss: 0.349666 Acc: 0.8438\n",
      "Train Epoch: 10 [48000/60000] Loss: 0.147049 Acc: 0.9688\n",
      "Train Epoch: 10 [51200/60000] Loss: 0.110496 Acc: 0.9688\n",
      "Train Epoch: 10 [54400/60000] Loss: 0.378573 Acc: 0.8438\n",
      "Train Epoch: 10 [57600/60000] Loss: 0.324248 Acc: 0.9375\n",
      "Elapsed 62.62s, 5.69 s/epoch, 0.00 s/batch, ets 51.24s\n",
      "\n",
      "Test set: Average loss: 0.2900, Accuracy: 8972/10000 (90%)\n",
      "\n",
      "Train Epoch: 11 [3200/60000] Loss: 0.085146 Acc: 1.0000\n",
      "Train Epoch: 11 [6400/60000] Loss: 0.364397 Acc: 0.8125\n",
      "Train Epoch: 11 [9600/60000] Loss: 0.161214 Acc: 0.9688\n",
      "Train Epoch: 11 [12800/60000] Loss: 0.378565 Acc: 0.7812\n",
      "Train Epoch: 11 [16000/60000] Loss: 0.188363 Acc: 0.9062\n",
      "Train Epoch: 11 [19200/60000] Loss: 0.119915 Acc: 0.9688\n",
      "Train Epoch: 11 [22400/60000] Loss: 0.097675 Acc: 1.0000\n",
      "Train Epoch: 11 [25600/60000] Loss: 0.061094 Acc: 1.0000\n",
      "Train Epoch: 11 [28800/60000] Loss: 0.491690 Acc: 0.8438\n",
      "Train Epoch: 11 [32000/60000] Loss: 0.058239 Acc: 1.0000\n",
      "Train Epoch: 11 [35200/60000] Loss: 0.230708 Acc: 0.9375\n",
      "Train Epoch: 11 [38400/60000] Loss: 0.341531 Acc: 0.9062\n",
      "Train Epoch: 11 [41600/60000] Loss: 0.088156 Acc: 1.0000\n",
      "Train Epoch: 11 [44800/60000] Loss: 0.303573 Acc: 0.8438\n",
      "Train Epoch: 11 [48000/60000] Loss: 0.198945 Acc: 0.9062\n",
      "Train Epoch: 11 [51200/60000] Loss: 0.145718 Acc: 0.9375\n",
      "Train Epoch: 11 [54400/60000] Loss: 0.095194 Acc: 1.0000\n",
      "Train Epoch: 11 [57600/60000] Loss: 0.372230 Acc: 0.8125\n",
      "Elapsed 68.61s, 5.72 s/epoch, 0.00 s/batch, ets 45.74s\n",
      "\n",
      "Test set: Average loss: 0.2882, Accuracy: 8964/10000 (90%)\n",
      "\n",
      "Train Epoch: 12 [3200/60000] Loss: 0.274748 Acc: 0.9062\n",
      "Train Epoch: 12 [6400/60000] Loss: 0.184992 Acc: 0.9375\n",
      "Train Epoch: 12 [9600/60000] Loss: 0.148868 Acc: 0.9688\n",
      "Train Epoch: 12 [12800/60000] Loss: 0.145933 Acc: 0.9062\n",
      "Train Epoch: 12 [16000/60000] Loss: 0.121500 Acc: 0.9375\n",
      "Train Epoch: 12 [19200/60000] Loss: 0.342179 Acc: 0.9375\n",
      "Train Epoch: 12 [22400/60000] Loss: 0.243621 Acc: 0.8750\n",
      "Train Epoch: 12 [25600/60000] Loss: 0.176039 Acc: 0.9062\n",
      "Train Epoch: 12 [28800/60000] Loss: 0.367489 Acc: 0.8438\n",
      "Train Epoch: 12 [32000/60000] Loss: 0.175734 Acc: 0.9375\n",
      "Train Epoch: 12 [35200/60000] Loss: 0.132101 Acc: 0.9375\n",
      "Train Epoch: 12 [38400/60000] Loss: 0.322414 Acc: 0.8750\n",
      "Train Epoch: 12 [41600/60000] Loss: 0.097525 Acc: 0.9688\n",
      "Train Epoch: 12 [44800/60000] Loss: 0.125855 Acc: 0.9375\n",
      "Train Epoch: 12 [48000/60000] Loss: 0.070247 Acc: 1.0000\n",
      "Train Epoch: 12 [51200/60000] Loss: 0.166478 Acc: 0.9688\n",
      "Train Epoch: 12 [54400/60000] Loss: 0.381467 Acc: 0.8438\n",
      "Train Epoch: 12 [57600/60000] Loss: 0.456129 Acc: 0.8125\n",
      "Elapsed 75.01s, 5.77 s/epoch, 0.00 s/batch, ets 40.39s\n",
      "\n",
      "Test set: Average loss: 0.2868, Accuracy: 8957/10000 (90%)\n",
      "\n",
      "Train Epoch: 13 [3200/60000] Loss: 0.213217 Acc: 0.9062\n",
      "Train Epoch: 13 [6400/60000] Loss: 0.233904 Acc: 0.9062\n",
      "Train Epoch: 13 [9600/60000] Loss: 0.121680 Acc: 1.0000\n",
      "Train Epoch: 13 [12800/60000] Loss: 0.364936 Acc: 0.9375\n",
      "Train Epoch: 13 [16000/60000] Loss: 0.117475 Acc: 0.9375\n",
      "Train Epoch: 13 [19200/60000] Loss: 0.210449 Acc: 0.8750\n",
      "Train Epoch: 13 [22400/60000] Loss: 0.140440 Acc: 0.9375\n",
      "Train Epoch: 13 [25600/60000] Loss: 0.285558 Acc: 0.9062\n",
      "Train Epoch: 13 [28800/60000] Loss: 0.103215 Acc: 0.9375\n",
      "Train Epoch: 13 [32000/60000] Loss: 0.232384 Acc: 0.8438\n",
      "Train Epoch: 13 [35200/60000] Loss: 0.615108 Acc: 0.7812\n",
      "Train Epoch: 13 [38400/60000] Loss: 0.185147 Acc: 0.9688\n",
      "Train Epoch: 13 [41600/60000] Loss: 0.231449 Acc: 0.8750\n",
      "Train Epoch: 13 [44800/60000] Loss: 0.022976 Acc: 1.0000\n",
      "Train Epoch: 13 [48000/60000] Loss: 0.168181 Acc: 0.8750\n",
      "Train Epoch: 13 [51200/60000] Loss: 0.344527 Acc: 0.9062\n",
      "Train Epoch: 13 [54400/60000] Loss: 0.277830 Acc: 0.8750\n",
      "Train Epoch: 13 [57600/60000] Loss: 0.142783 Acc: 0.9375\n",
      "Elapsed 81.24s, 5.80 s/epoch, 0.00 s/batch, ets 34.82s\n",
      "\n",
      "Test set: Average loss: 0.2900, Accuracy: 8968/10000 (90%)\n",
      "\n",
      "Train Epoch: 14 [3200/60000] Loss: 0.170996 Acc: 0.9062\n",
      "Train Epoch: 14 [6400/60000] Loss: 0.382293 Acc: 0.8750\n",
      "Train Epoch: 14 [9600/60000] Loss: 0.328789 Acc: 0.9062\n",
      "Train Epoch: 14 [12800/60000] Loss: 0.103181 Acc: 0.9688\n",
      "Train Epoch: 14 [16000/60000] Loss: 0.264214 Acc: 0.8750\n",
      "Train Epoch: 14 [19200/60000] Loss: 0.112154 Acc: 0.9375\n",
      "Train Epoch: 14 [22400/60000] Loss: 0.615966 Acc: 0.8125\n",
      "Train Epoch: 14 [25600/60000] Loss: 0.186163 Acc: 0.9375\n",
      "Train Epoch: 14 [28800/60000] Loss: 0.348667 Acc: 0.8750\n",
      "Train Epoch: 14 [32000/60000] Loss: 0.179304 Acc: 0.9062\n",
      "Train Epoch: 14 [35200/60000] Loss: 0.318897 Acc: 0.9062\n",
      "Train Epoch: 14 [38400/60000] Loss: 0.221001 Acc: 0.8750\n",
      "Train Epoch: 14 [41600/60000] Loss: 0.148868 Acc: 0.9688\n",
      "Train Epoch: 14 [44800/60000] Loss: 0.201801 Acc: 0.9375\n",
      "Train Epoch: 14 [48000/60000] Loss: 0.072931 Acc: 1.0000\n",
      "Train Epoch: 14 [51200/60000] Loss: 0.238155 Acc: 0.8750\n",
      "Train Epoch: 14 [54400/60000] Loss: 0.223375 Acc: 0.9062\n",
      "Train Epoch: 14 [57600/60000] Loss: 0.256183 Acc: 0.9062\n",
      "Elapsed 87.27s, 5.82 s/epoch, 0.00 s/batch, ets 29.09s\n",
      "\n",
      "Test set: Average loss: 0.2885, Accuracy: 8962/10000 (90%)\n",
      "\n",
      "Train Epoch: 15 [3200/60000] Loss: 0.339128 Acc: 0.8750\n",
      "Train Epoch: 15 [6400/60000] Loss: 0.137026 Acc: 0.9375\n",
      "Train Epoch: 15 [9600/60000] Loss: 0.080756 Acc: 0.9688\n",
      "Train Epoch: 15 [12800/60000] Loss: 0.213855 Acc: 0.9375\n",
      "Train Epoch: 15 [16000/60000] Loss: 0.225258 Acc: 0.9375\n",
      "Train Epoch: 15 [19200/60000] Loss: 0.143665 Acc: 0.9688\n",
      "Train Epoch: 15 [22400/60000] Loss: 0.373047 Acc: 0.9062\n",
      "Train Epoch: 15 [25600/60000] Loss: 0.166875 Acc: 0.9375\n",
      "Train Epoch: 15 [28800/60000] Loss: 0.447765 Acc: 0.8125\n",
      "Train Epoch: 15 [32000/60000] Loss: 0.202252 Acc: 0.9062\n",
      "Train Epoch: 15 [35200/60000] Loss: 0.206058 Acc: 0.9062\n",
      "Train Epoch: 15 [38400/60000] Loss: 0.178745 Acc: 0.9375\n",
      "Train Epoch: 15 [41600/60000] Loss: 0.421097 Acc: 0.9062\n",
      "Train Epoch: 15 [44800/60000] Loss: 0.488084 Acc: 0.8125\n",
      "Train Epoch: 15 [48000/60000] Loss: 0.378418 Acc: 0.9375\n",
      "Train Epoch: 15 [51200/60000] Loss: 0.195416 Acc: 0.9062\n",
      "Train Epoch: 15 [54400/60000] Loss: 0.129861 Acc: 0.9375\n",
      "Train Epoch: 15 [57600/60000] Loss: 0.139576 Acc: 0.9375\n",
      "Elapsed 93.16s, 5.82 s/epoch, 0.00 s/batch, ets 23.29s\n",
      "\n",
      "Test set: Average loss: 0.3008, Accuracy: 8935/10000 (89%)\n",
      "\n",
      "Train Epoch: 16 [3200/60000] Loss: 0.185743 Acc: 0.9062\n",
      "Train Epoch: 16 [6400/60000] Loss: 0.227154 Acc: 0.9062\n",
      "Train Epoch: 16 [9600/60000] Loss: 0.179735 Acc: 0.9375\n",
      "Train Epoch: 16 [12800/60000] Loss: 0.106438 Acc: 0.9688\n",
      "Train Epoch: 16 [16000/60000] Loss: 0.236183 Acc: 0.9062\n",
      "Train Epoch: 16 [19200/60000] Loss: 0.306014 Acc: 0.9062\n",
      "Train Epoch: 16 [22400/60000] Loss: 0.096730 Acc: 0.9688\n",
      "Train Epoch: 16 [25600/60000] Loss: 0.378413 Acc: 0.8438\n",
      "Train Epoch: 16 [28800/60000] Loss: 0.389344 Acc: 0.9062\n",
      "Train Epoch: 16 [32000/60000] Loss: 0.154202 Acc: 0.9375\n",
      "Train Epoch: 16 [35200/60000] Loss: 0.163712 Acc: 0.9062\n",
      "Train Epoch: 16 [38400/60000] Loss: 0.311947 Acc: 0.8750\n",
      "Train Epoch: 16 [41600/60000] Loss: 0.168811 Acc: 0.9375\n",
      "Train Epoch: 16 [44800/60000] Loss: 0.212425 Acc: 0.9062\n",
      "Train Epoch: 16 [48000/60000] Loss: 0.178519 Acc: 0.9375\n",
      "Train Epoch: 16 [51200/60000] Loss: 0.069957 Acc: 0.9688\n",
      "Train Epoch: 16 [54400/60000] Loss: 0.195833 Acc: 0.9688\n",
      "Train Epoch: 16 [57600/60000] Loss: 0.118252 Acc: 0.9375\n",
      "Elapsed 99.45s, 5.85 s/epoch, 0.00 s/batch, ets 17.55s\n",
      "\n",
      "Test set: Average loss: 0.2932, Accuracy: 8952/10000 (90%)\n",
      "\n",
      "Train Epoch: 17 [3200/60000] Loss: 0.308999 Acc: 0.8438\n",
      "Train Epoch: 17 [6400/60000] Loss: 0.107246 Acc: 1.0000\n",
      "Train Epoch: 17 [9600/60000] Loss: 0.228263 Acc: 0.8750\n",
      "Train Epoch: 17 [12800/60000] Loss: 0.278174 Acc: 0.8750\n",
      "Train Epoch: 17 [16000/60000] Loss: 0.097399 Acc: 0.9375\n",
      "Train Epoch: 17 [19200/60000] Loss: 0.096144 Acc: 0.9375\n",
      "Train Epoch: 17 [22400/60000] Loss: 0.155484 Acc: 0.9375\n",
      "Train Epoch: 17 [25600/60000] Loss: 0.094178 Acc: 1.0000\n",
      "Train Epoch: 17 [28800/60000] Loss: 0.284685 Acc: 0.9375\n",
      "Train Epoch: 17 [32000/60000] Loss: 0.082844 Acc: 1.0000\n",
      "Train Epoch: 17 [35200/60000] Loss: 0.145334 Acc: 0.9688\n",
      "Train Epoch: 17 [38400/60000] Loss: 0.088621 Acc: 0.9688\n",
      "Train Epoch: 17 [41600/60000] Loss: 0.097231 Acc: 0.9688\n",
      "Train Epoch: 17 [44800/60000] Loss: 0.388256 Acc: 0.9062\n",
      "Train Epoch: 17 [48000/60000] Loss: 0.263413 Acc: 0.8750\n",
      "Train Epoch: 17 [51200/60000] Loss: 0.199417 Acc: 0.9375\n",
      "Train Epoch: 17 [54400/60000] Loss: 0.250382 Acc: 0.9062\n",
      "Train Epoch: 17 [57600/60000] Loss: 0.349475 Acc: 0.9062\n",
      "Elapsed 106.02s, 5.89 s/epoch, 0.00 s/batch, ets 11.78s\n",
      "\n",
      "Test set: Average loss: 0.2825, Accuracy: 8990/10000 (90%)\n",
      "\n",
      "Train Epoch: 18 [3200/60000] Loss: 0.227172 Acc: 0.8438\n",
      "Train Epoch: 18 [6400/60000] Loss: 0.145650 Acc: 0.9688\n",
      "Train Epoch: 18 [9600/60000] Loss: 0.342416 Acc: 0.8438\n",
      "Train Epoch: 18 [12800/60000] Loss: 0.039517 Acc: 1.0000\n",
      "Train Epoch: 18 [16000/60000] Loss: 0.140330 Acc: 0.9375\n",
      "Train Epoch: 18 [19200/60000] Loss: 0.107480 Acc: 0.9375\n",
      "Train Epoch: 18 [22400/60000] Loss: 0.103558 Acc: 0.9688\n",
      "Train Epoch: 18 [25600/60000] Loss: 0.240244 Acc: 0.8750\n",
      "Train Epoch: 18 [28800/60000] Loss: 0.124633 Acc: 0.9375\n",
      "Train Epoch: 18 [32000/60000] Loss: 0.164054 Acc: 0.9375\n",
      "Train Epoch: 18 [35200/60000] Loss: 0.149928 Acc: 0.9688\n",
      "Train Epoch: 18 [38400/60000] Loss: 0.060593 Acc: 1.0000\n",
      "Train Epoch: 18 [41600/60000] Loss: 0.202574 Acc: 0.9062\n",
      "Train Epoch: 18 [44800/60000] Loss: 0.197340 Acc: 0.8750\n",
      "Train Epoch: 18 [48000/60000] Loss: 0.071285 Acc: 0.9688\n",
      "Train Epoch: 18 [51200/60000] Loss: 0.221517 Acc: 0.9688\n",
      "Train Epoch: 18 [54400/60000] Loss: 0.297170 Acc: 0.8750\n",
      "Train Epoch: 18 [57600/60000] Loss: 0.212772 Acc: 0.8750\n",
      "Elapsed 113.22s, 5.96 s/epoch, 0.00 s/batch, ets 5.96s\n",
      "\n",
      "Test set: Average loss: 0.3216, Accuracy: 8890/10000 (89%)\n",
      "\n",
      "Train Epoch: 19 [3200/60000] Loss: 0.343107 Acc: 0.9062\n",
      "Train Epoch: 19 [6400/60000] Loss: 0.171193 Acc: 0.9375\n",
      "Train Epoch: 19 [9600/60000] Loss: 0.151671 Acc: 0.9062\n",
      "Train Epoch: 19 [12800/60000] Loss: 0.124439 Acc: 0.9375\n",
      "Train Epoch: 19 [16000/60000] Loss: 0.287532 Acc: 0.9688\n",
      "Train Epoch: 19 [19200/60000] Loss: 0.237697 Acc: 0.9375\n",
      "Train Epoch: 19 [22400/60000] Loss: 0.070175 Acc: 0.9688\n",
      "Train Epoch: 19 [25600/60000] Loss: 0.238083 Acc: 0.9375\n",
      "Train Epoch: 19 [28800/60000] Loss: 0.174871 Acc: 0.9062\n",
      "Train Epoch: 19 [32000/60000] Loss: 0.193101 Acc: 0.9062\n",
      "Train Epoch: 19 [35200/60000] Loss: 0.208882 Acc: 0.8438\n",
      "Train Epoch: 19 [38400/60000] Loss: 0.199038 Acc: 0.8438\n",
      "Train Epoch: 19 [41600/60000] Loss: 0.239992 Acc: 0.9062\n",
      "Train Epoch: 19 [44800/60000] Loss: 0.379224 Acc: 0.9375\n",
      "Train Epoch: 19 [48000/60000] Loss: 0.136897 Acc: 0.9375\n",
      "Train Epoch: 19 [51200/60000] Loss: 0.128611 Acc: 0.9688\n",
      "Train Epoch: 19 [54400/60000] Loss: 0.233642 Acc: 0.9062\n",
      "Train Epoch: 19 [57600/60000] Loss: 0.199255 Acc: 0.8750\n",
      "Elapsed 118.83s, 5.94 s/epoch, 0.00 s/batch, ets 0.00s\n",
      "\n",
      "Test set: Average loss: 0.2924, Accuracy: 8988/10000 (90%)\n",
      "\n",
      "Total time: 119.47, Best Loss: 0.283\n"
     ]
    }
   ],
   "source": [
    "model = LeNet()\n",
    "modelBN = LeNetBN() \n",
    "\n",
    "model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc = main(model)\n",
    "\n",
    "modelBN, epoch_train_loss_bn, epoch_train_acc_bn, epoch_test_loss_bn, epoch_test_acc_bn = main(modelBN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:green\">10. Plot Loss</font> <a name=\"plot-loss\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot loss\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "x = range(len(epoch_train_loss))\n",
    "\n",
    "\n",
    "plt.figure\n",
    "plt.plot(x, epoch_train_loss, 'r', label=\"train loss\")\n",
    "plt.plot(x, epoch_test_loss, 'b', label=\"validation loss\")\n",
    "\n",
    "plt.plot(x, epoch_train_loss_bn, 'r--', label=\"train loss with BN\")\n",
    "plt.plot(x, epoch_test_loss_bn, 'b--',label=\"validation loss with BN\")\n",
    "\n",
    "plt.xlabel('epoch no.')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above curves show that when we use Batch Normalization, the training converges faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font style=\"color:pink\">Miscellaneous: Calculate Mean and Standard Deviation of Fashion MNIST </font> <a name=\"misc\"></a>\n",
    "\n",
    "Ideally, we should not use the same mean and standard deviation for Fashion MNIST and MNIST. Refrain, even when you find many continuing to do this, simply because it does not have a profound effect on the results.\n",
    "\n",
    "Let us find  the mean and standard deviation for Fashion MNIST and use it instead of MNIST.\n",
    "\n",
    "We need to simply find the mean and standard deviation of the whole dataset. So, we load the dataset and then use the functions given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2860)\n",
      "tensor(0.3530)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "train_transform = transforms.Compose([transforms.ToTensor()])\n",
    "train_set = torchvision.datasets.FashionMNIST(root=\"./data\", train=True, download=True, transform=train_transform)\n",
    "\n",
    "print(train_set.data.float().mean()/255)\n",
    "print(train_set.data.float().std()/255)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
