{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <font style=\"color:blue\">Assignment: LinkNet Architecture with VGG16<\/font>\n",
                "\n",
                "You have already implemented the `LinkNet` with backbone `ResNet18`, earlier in the course. Here, you have to do the same with `VGG16` backbone.\n",
                "\n",
                "## <font color='blue'>Marking Scheme<\/font>\n",
                "\n",
                "#### Maximum Points: 30\n",
                "\n",
                "<div>\n",
                "    <table>\n",
                "        <tr><td><h3>Sr. no.<\/h3><\/td> <td><h3>Problem<\/h3><\/td> <td><h3>Points<\/h3><\/td> <\/tr>\n",
                "        <tr><td><h3>1<\/h3><\/td> <td><h3>3. LinkNet Implementation<\/h3><\/td> <td><h3>30<\/h3><\/td> <\/tr>\n",
                "    <\/table>\n",
                "<\/div>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <font style=\"color:green\">1. LinkNet<\/font>\n",
                "\n",
                "Let's briefly overview the LinkNet architecture. [LinkNet](https:\/\/arxiv.org\/pdf\/1707.03718.pdf) was\n",
                "introduced in 2017 by A.Chaurasia and E.Culurciello as a novel lightweight deep neural network for semantic\n",
                "segmentation.\n",
                "\n",
                "---\n",
                "\n",
                "<img src='https:\/\/www.learnopencv.com\/wp-content\/uploads\/2020\/04\/c3-w11-LinkNet_architecture.png'>\n",
                "\n",
                "---\n",
                "\n",
                "In the above picture: \n",
                "- `\/2` means downsampling of the feature map by a factor of `2`, which is achieved by performing strided convolution\n",
                "- `\u22172` denotes upsampling by `2`\n",
                "- The left half of the network is an encoder, whereas  the right side of it is a decoder."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <font style=\"color:green\">2. Implementation Guidelines <\/font>\n",
                "\n",
                "\n",
                "---\n",
                "\n",
                "<img src='https:\/\/www.learnopencv.com\/wp-content\/uploads\/2020\/04\/c3-w11-VGG16-architecture-16.png'>\n",
                "<center>Image credits: www.researchgate.net<\/center>\n",
                "\n",
                "\n",
                "---\n",
                "\n",
                "**1. The following block (before encoder) will change as follows for VGG16:**\n",
                "\n",
                "<img src='https:\/\/www.learnopencv.com\/wp-content\/uploads\/2020\/04\/c3-w11-assignment-1.png'>\n",
                "\n",
                "- `conv[(7x7), (3, 64), \/2]` will be replaced by the first two convolution layers (`convolution + ReLU`) of `VGG16`. <br>\n",
                "The conv layers are of type `conv((3, 3), (3, 64))` and `conv((3, 3), (64, 64))`\n",
                "\n",
                "\n",
                "- `max-pool[(3x3), \/2]` will be replaced by the first max-pool layer of `VGG16` (`max-pool[(2x2), \/2]`).\n",
                "\n",
                "\n",
                "**2. Sub-sequent convolution layers till max-pool (including max-pool) will be used as one encoder block. For example, fourth (`convolution + ReLU`), fifth (`convolution + ReLU`), and sixth (`max pooling`) layers combined will be used as `Encoder Block 1`.**\n",
                "\n",
                "\n",
                "**3. The decoder code is already given, you don't have to change anything.**\n",
                "\n",
                "**4. The number of output channels of `decoder block 1` must be `64` (Though logically, it can be any number).**\n",
                "\n",
                "\n",
                "**5. The block after `decoder block 1`, let's call it `classifier`. Write code for the block, such that it takes care of the number of classes as well as the input image width and height (output size of the classifier = `[batch_size, num_classes, height, width]`)**\n",
                "\n",
                "\n",
                "<img src='https:\/\/www.learnopencv.com\/wp-content\/uploads\/2020\/04\/c3-w11-assignment2.png'>\n",
                "\n",
                "- Replace the above block with your classifier. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <font style=\"color:green\">3. LinkNet Implementation [30 Points]<\/font>\n",
                "\n",
                "\n",
                "**The LinkNet implementation points divided into two:**\n",
                "\n",
                "<p><\/p>\n",
                "\n",
                "<div>\n",
                "    <table>\n",
                "        <tr><td><h3>Sr. no.<\/h3><\/td> <td><h3>Problem<\/h3><\/td> <td><h3>Points<\/h3><\/td> <\/tr>\n",
                "        <tr><td><h3>1<\/h3><\/td> <td><h3>Encoder-Decoder Implementation<\/h3><\/td> <td><h3>15<\/h3><\/td> <\/tr>\n",
                "        <tr><td><h3>2<\/h3><\/td> <td><h3>Classifier & Forward Implementation<\/h3><\/td> <td><h3>15<\/h3><\/td> <\/tr>\n",
                "    <\/table>\n",
                "<\/div>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "import torch\n",
                "# torch neural network (NN) module for building and training nets\n",
                "import torch.nn as nn\n",
                "# module with various model definitions\n",
                "import torchvision.models as models\n",
                "\n",
                "import random"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:green\">3.1. Decoder<\/font>\n",
                "\n",
                "The block below is a decoder, which takes a feature map with defined channel numbers. The `channels_in` the result map should be equal to `channels_out`.\n",
                "\n",
                "We have used `ConvTranspose2d` for upsampling. Find details [here](https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.ConvTranspose2d)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# create decoder block inherited from nn.Module\n",
                "class DecoderBlock(nn.Module):\n",
                "    def __init__(self, channels_in, channels_out):\n",
                "        super().__init__()\n",
                "\n",
                "        # 1x1 projection module to reduce channels\n",
                "        self.proj = nn.Sequential(\n",
                "            # convolution\n",
                "            nn.Conv2d(channels_in, channels_in \/\/ 2, kernel_size=1, bias=False),\n",
                "            # batch normalization\n",
                "            nn.BatchNorm2d(channels_in \/\/ 2),\n",
                "            # relu activation\n",
                "            nn.ReLU()\n",
                "        )\n",
                "\n",
                "        # fully convolutional module\n",
                "        self.deconv = nn.Sequential(\n",
                "            # deconvolution\n",
                "            nn.ConvTranspose2d(\n",
                "                channels_in \/\/ 2,\n",
                "                channels_in \/\/ 2,\n",
                "                kernel_size=2,\n",
                "                stride=2,\n",
                "                padding=0,\n",
                "                output_padding=0,\n",
                "                groups=channels_in \/\/ 2,\n",
                "                bias=False\n",
                "            ),\n",
                "            # batch normalization\n",
                "            nn.BatchNorm2d(channels_in \/\/ 2),\n",
                "            # relu activation\n",
                "            nn.ReLU()\n",
                "        )\n",
                "\n",
                "        # 1x1 unprojection module to increase channels\n",
                "        self.unproj = nn.Sequential(\n",
                "            # convolution\n",
                "            nn.Conv2d(channels_in \/\/ 2, channels_out, kernel_size=1, bias=False),\n",
                "            # batch normalization\n",
                "            nn.BatchNorm2d(channels_out),\n",
                "            # relu activation\n",
                "            nn.ReLU()\n",
                "        )\n",
                "\n",
                "    # stack layers and perform a forward pass\n",
                "    def forward(self, x):\n",
                "\n",
                "        proj = self.proj(x)\n",
                "        deconv = self.deconv(proj)\n",
                "        unproj = self.unproj(deconv)\n",
                "\n",
                "        return unproj"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:green\">3.2. LinkNet<\/font>\n",
                "\n",
                "**Write your code where it is specified. Do not modify\/delete other codes.**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "# create LinkNet model with VGG16 encoder\n",
                "class LinkNet(nn.Module):\n",
                "    def __init__(self, num_classes, encoder=\"vgg16\"):\n",
                "        super().__init__()\n",
                "        assert hasattr(models, encoder), \"Undefined encoder type\"\n",
                "        # prepare feature extractor from `torchvision` vgg16 model\n",
                "        \n",
                "        vgg16 = getattr(models, encoder)(pretrained=False)\n",
                "        \n",
                "        ###########################################################################################\n",
                "        # write code for self.init and self.maxpool\n",
                "        # for self.init: the first and second convolution layers (including relu) of VGG16.\n",
                "        # for self.maxpool: the first max-pool layer of VGG16 (max-pool[(2x2), \/2]).\n",
                "        ###########################################################################################\n",
                "        \n",
                "        self.init = None\n",
                "        self.maxpool = None\n",
                "        \n",
                "        ###\n",
                "        ### YOUR CODE HERE\n",
                "        ###\n",
                "        \n",
                "        ############################################################################################\n",
                "        # Write code for encoder blocks:\n",
                "        # Sub-sequent convolution layers and max-pool combined will be used as encoder blocks. \n",
                "        # Let's name encoder blocks as self.layer1, self.layer2, self.layer3, and self.layer4 for \n",
                "        # encoder block1, encoder block2, encoder block3 and encoder block4 respectively.\n",
                "        ############################################################################################\n",
                "        \n",
                "        self.layer1 = None\n",
                "        self.layer2 = None\n",
                "        self.layer3 = None\n",
                "        self.layer4 = None    \n",
                "        \n",
                "        ###\n",
                "        ### YOUR CODE HERE\n",
                "        ###\n",
                "\n",
                "        \n",
                "        #############################################################################################\n",
                "        # Decoder's block: DecoderBlock module\n",
                "        \n",
                "        # Write code for decoder block here. As DecoderBlock class is already defined, you have to \n",
                "        # initiate the class with arguments channels_in and channels_out. \n",
                "        \n",
                "        # Let's name decoder block as self.up4, self.up3, self.up2 and self.up1 for decoder block4, \n",
                "        # decoder block3, decoder block2 and decoder block1 respectively. \n",
                "        #############################################################################################\n",
                "        \n",
                "        self.up4 = None\n",
                "        self.up3 = None\n",
                "        self.up2 = None\n",
                "        \n",
                "        # output channel of self.up1 must be 64\n",
                "        self.up1 = None     \n",
                "\n",
                "        ###\n",
                "        ### YOUR CODE HERE\n",
                "        ###\n",
                "\n",
                "        # Classification block: define a classifier module\n",
                "        \n",
                "        ################################################################################################\n",
                "        # You have to write the classifier part as a Sequential model.\n",
                "        ################################################################################################\n",
                "        self.classifier = nn.Sequential(\n",
                "            ###\n",
                "            ### YOUR CODE HERE\n",
                "            ###\n",
                "        )\n",
                "\n",
                "\n",
                "    # define the forward pass\n",
                "    def forward(self, x):\n",
                "        \n",
                "        #############################################################################################\n",
                "        # You have to complete the forward method for LinkNet.\n",
                "        #############################################################################################\n",
                "        \n",
                "        # for input image size (3, 320, 320)\n",
                "\n",
                "        # output size = (64, 320, 320)\n",
                "        init = self.init(x)\n",
                "        # output size = (64, 160, 160)\n",
                "        maxpool = self.maxpool(init)\n",
                "        \n",
                "        ###\n",
                "        ### YOUR CODE HERE\n",
                "        ###\n",
                "\n",
                "        # output size = (5, 320, 320), where 5 is the predefined number of classes\n",
                "        output = self.classifier(up1)\n",
                "\n",
                "        return output"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# <font style=\"color:green\">4. Check the implementation<\/font>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:green\">4.1. Check Encoder-Decoder Implementation With Model Profiler<\/font>\n",
                "\n",
                "Verify your encoder-decoder implementation with the model profiler before submitting it. \n",
                "Here, we will check the number of floating-points operations and of parameters. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "class ModelProfiler(nn.Module):\n",
                "    \"\"\" Profile PyTorch models.\n",
                "\n",
                "    Compute FLOPs (FLoating OPerations) and number of trainable parameters of model.\n",
                "\n",
                "    Arguments:\n",
                "        model (nn.Module): model which will be profiled.\n",
                "\n",
                "    Example:\n",
                "        model = torchvision.models.resnet50()\n",
                "        profiler = ModelProfiler(model)\n",
                "        var = torch.zeros(1, 3, 224, 224)\n",
                "        profiler(var)\n",
                "        print(\"FLOPs: {0:.5}; #Params: {1:.5}\".format(profiler.get_flops('G'), profiler.get_params('M')))\n",
                "\n",
                "    Warning:\n",
                "        Model profiler doesn't work with models, wrapped by torch.nn.DataParallel.\n",
                "    \"\"\"\n",
                "    def __init__(self, model):\n",
                "        super().__init__()\n",
                "        self.model = model\n",
                "        self.flops = 0\n",
                "        self.units = {'K': 10.**3, 'M': 10.**6, 'G': 10.**9}\n",
                "        self.hooks = None\n",
                "        self._remove_hooks()\n",
                "\n",
                "    def get_flops(self, units='G'):\n",
                "        \"\"\" Get number of floating operations per inference.\n",
                "\n",
                "        Arguments:\n",
                "            units (string): units of the flops value ('K': Kilo (10^3), 'M': Mega (10^6), 'G': Giga (10^9)).\n",
                "\n",
                "        Returns:\n",
                "            Floating operations per inference at the chosen units.\n",
                "        \"\"\"\n",
                "        assert units in self.units\n",
                "        return self.flops \/ self.units[units]\n",
                "\n",
                "    def get_params(self, units='G'):\n",
                "        \"\"\" Get number of trainable parameters of the model.\n",
                "\n",
                "        Arguments:\n",
                "            units (string): units of the flops value ('K': Kilo (10^3), 'M': Mega (10^6), 'G': Giga (10^9)).\n",
                "\n",
                "        Returns:\n",
                "            Number of trainable parameters of the model at the chosen units.\n",
                "        \"\"\"\n",
                "        assert units in self.units\n",
                "        params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
                "        if units is not None:\n",
                "            params = params \/ self.units[units]\n",
                "        return params\n",
                "\n",
                "    def forward(self, *args, **kwargs):\n",
                "        self.flops = 0\n",
                "        self._init_hooks()\n",
                "        output = self.model(*args, **kwargs)\n",
                "        self._remove_hooks()\n",
                "        return output\n",
                "\n",
                "    def _remove_hooks(self):\n",
                "        if self.hooks is not None:\n",
                "            for hook in self.hooks:\n",
                "                hook.remove()\n",
                "        self.hooks = None\n",
                "\n",
                "    def _init_hooks(self):\n",
                "        self.hooks = []\n",
                "\n",
                "        def hook_compute_flop(module, _, output):\n",
                "            self.flops += module.weight.size()[1:].numel() * output.size()[1:].numel()\n",
                "\n",
                "        def add_hooks(module):\n",
                "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
                "                self.hooks.append(module.register_forward_hook(hook_compute_flop))\n",
                "\n",
                "        self.model.apply(add_hooks)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "def profile_model(model, input_size, cuda):\n",
                "    \"\"\" Compute FLOPS and #Params of the CNN.\n",
                "\n",
                "    Arguments:\n",
                "        model (nn.Module): model which should be profiled.\n",
                "        input_size (tuple): size of the input variable.\n",
                "        cuda (bool): if True then variable will be upload to the GPU.\n",
                "\n",
                "    Returns:\n",
                "        dict:\n",
                "            dict[\"flops\"] (float): number of GFLOPs.\n",
                "            dict[\"params\"] (int): number of million parameters.\n",
                "    \"\"\"\n",
                "    profiler = ModelProfiler(model)\n",
                "    var = torch.zeros(input_size)\n",
                "    if cuda:\n",
                "        var = var.cuda()\n",
                "    profiler(var)\n",
                "    return {\"flops\": profiler.get_flops('G'), \"params\": profiler.get_params('M')}"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "collapsed": true
            },
            "outputs": [],
            "source": [
                "def GFLOPs_and_Parms_count_summary(block, input_tensor, block_name):\n",
                "    \n",
                "    flops, params = profile_model(block, input_tensor.size(), False).values()\n",
                "    \n",
                "    print('{0}\\n{1}\\n{0}'.format('-'*50, block_name))\n",
                "\n",
                "    print('GFLOPs:\\t\\t\\t\\t{}\\nNo. of params (in million):\\t{}'.format(flops, params))\n",
                "    \n",
                "    return block(input_tensor)\n",
                "\n",
                "    "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Running the below cell, you should get the following outputs:**\n",
                "\n",
                "```\n",
                "--------------------------------------------------\n",
                "init\n",
                "--------------------------------------------------\n",
                "GFLOPs:\t\t\t\t        3.9518208\n",
                "No. of params (in million):\t0.03872\n",
                "--------------------------------------------------\n",
                "maxpool\n",
                "--------------------------------------------------\n",
                "GFLOPs:\t\t\t\t        0.0\n",
                "No. of params (in million):\t0.0\n",
                "--------------------------------------------------\n",
                "layer1\n",
                "--------------------------------------------------\n",
                "GFLOPs:\t\t\t\t        5.6623104\n",
                "No. of params (in million):\t0.22144\n",
                "--------------------------------------------------\n",
                "layer2\n",
                "--------------------------------------------------\n",
                "GFLOPs:\t\t\t\t        9.437184\n",
                "No. of params (in million):\t1.475328\n",
                "--------------------------------------------------\n",
                "layer3\n",
                "--------------------------------------------------\n",
                "GFLOPs:\t\t\t\t        9.437184\n",
                "No. of params (in million):\t5.899776\n",
                "--------------------------------------------------\n",
                "layer4\n",
                "--------------------------------------------------\n",
                "GFLOPs:\t\t\t\t        2.8311552\n",
                "No. of params (in million):\t7.079424\n",
                "--------------------------------------------------\n",
                "up4\n",
                "--------------------------------------------------\n",
                "GFLOPs:\t\t\t\t        0.065536\n",
                "No. of params (in million):\t0.265216\n",
                "--------------------------------------------------\n",
                "up3\n",
                "--------------------------------------------------\n",
                "GFLOPs:\t\t\t\t        0.1572864\n",
                "No. of params (in million):\t0.199168\n",
                "--------------------------------------------------\n",
                "up2\n",
                "--------------------------------------------------\n",
                "GFLOPs:\t\t\t\t        0.1572864\n",
                "No. of params (in million):\t0.050432\n",
                "--------------------------------------------------\n",
                "up1\n",
                "--------------------------------------------------\n",
                "GFLOPs:\t\t\t\t        0.1572864\n",
                "No. of params (in million):\t0.012928\n",
                "```\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "# input data for model check\n",
                "input_tensor = torch.zeros(1, 3, 320, 320)\n",
                "\n",
                "# LinkNet architecture\n",
                "model = LinkNet(num_classes=5, encoder=\"vgg16\")\n",
                "\n",
                "input_tensor = GFLOPs_and_Parms_count_summary(model.init, input_tensor, 'init')\n",
                "input_tensor = GFLOPs_and_Parms_count_summary(model.maxpool, input_tensor, 'maxpool')\n",
                "input_tensor = GFLOPs_and_Parms_count_summary(model.layer1, input_tensor, 'layer1')\n",
                "input_tensor = GFLOPs_and_Parms_count_summary(model.layer2, input_tensor, 'layer2')\n",
                "input_tensor = GFLOPs_and_Parms_count_summary(model.layer3, input_tensor, 'layer3')\n",
                "input_tensor = GFLOPs_and_Parms_count_summary(model.layer4, input_tensor, 'layer4')\n",
                "input_tensor = GFLOPs_and_Parms_count_summary(model.up4, input_tensor, 'up4')\n",
                "input_tensor = GFLOPs_and_Parms_count_summary(model.up3, input_tensor, 'up3')\n",
                "input_tensor = GFLOPs_and_Parms_count_summary(model.up2, input_tensor, 'up2')\n",
                "input_tensor = GFLOPs_and_Parms_count_summary(model.up1, input_tensor, 'up1')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "Encoder-Decoder Implementation",
                    "locked": true,
                    "points": "15",
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## <font style=\"color:green\">4.2. Check classifier & forward Implementation<\/font>\n",
                "\n",
                "**Input width and height is the same as output width and height because semantic segmentation predicts the label of each pixel.**\n",
                "\n",
                "**Running the below cell, you should get the following outputs:**\n",
                "```\n",
                "Prediction Size: torch.Size([1, 5, 320, 320])\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "# input data for model check\n",
                "input_tensor = torch.zeros(1, 3, 320, 320)\n",
                "\n",
                "# LinkNet architecture\n",
                "model = LinkNet(num_classes=5, encoder=\"vgg16\")\n",
                "\n",
                "# examining the prediction size\n",
                "pred = model(input_tensor)\n",
                "print('Prediction Size: {}'.format(pred.size()))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "collapsed": true,
                "nbgrader": {
                    "grade": true,
                    "grade_id": "Classifier & Forward Implementation",
                    "locked": true,
                    "points": "15",
                    "solution": false
                },
                "editable": false,
                "deletable": false
            },
            "outputs": [],
            "source": [
                "###\n",
                "### AUTOGRADER TEST - DO NOT REMOVE\n",
                "###\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8",
            "language": "python",
            "name": "python38"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text\/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.7"
        },
        "toc": {
            "base_numbering": 1,
            "nav_menu": [],
            "number_sections": false,
            "sideBar": true,
            "skip_h1_title": false,
            "title_cell": "Table of Contents",
            "title_sidebar": "Contents",
            "toc_cell": false,
            "toc_position": [],
            "toc_section_display": true,
            "toc_window_display": false
        },
        "varInspector": {
            "cols": {
                "lenName": 16,
                "lenType": 16,
                "lenVar": 40
            },
            "kernels_config": {
                "python": {
                    "delete_cmd_postfix": "",
                    "delete_cmd_prefix": "del ",
                    "library": "var_list.py",
                    "varRefreshCmd": "print(var_dic_list())"
                },
                "r": {
                    "delete_cmd_postfix": ") ",
                    "delete_cmd_prefix": "rm(",
                    "library": "var_list.r",
                    "varRefreshCmd": "cat(var_dic_list()) "
                }
            },
            "types_to_exclude": [
                "module",
                "function",
                "builtin_function_or_method",
                "instance",
                "_Feature"
            ],
            "window_display": false
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}